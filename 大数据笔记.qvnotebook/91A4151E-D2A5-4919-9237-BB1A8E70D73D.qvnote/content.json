{
  "title": "10-Storm-0301-Storm和Hbase的集成",
  "cells": [
    {
      "type": "markdown",
      "data": "*storm和hbase的集成最好自己写一个bolt，在bolt中建立hbase连接，通过表进行存入，具体如下*"
    },
    {
      "type": "markdown",
      "data": "* 01，配置pom.xml文件 \n\n  ```xml\n  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n  <project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n           xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n           xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n      <modelVersion>4.0.0</modelVersion>\n  \n      <groupId>im.ivanl001</groupId>\n      <artifactId>10-Storm</artifactId>\n      <version>1.0-SNAPSHOT</version>\n  \n      <packaging>jar</packaging>\n  \n      <!--这他妈要求也太高了， 版本不对也不行，这怎么玩，要是升级了我怎么知道是哪个版本的？？？？？-->\n      <dependencies>\n          <dependency>\n              <groupId>org.apache.storm</groupId>\n              <artifactId>storm-core</artifactId>\n              <version>1.0.3</version>\n          </dependency>\n          <dependency>\n              <groupId>junit</groupId>\n              <artifactId>junit</artifactId>\n              <version>4.11</version>\n          </dependency>\n          <dependency>\n              <groupId>org.apache.storm</groupId>\n              <artifactId>storm-kafka</artifactId>\n              <version>1.0.2</version>\n          </dependency>\n          <dependency>\n              <groupId>log4j</groupId>\n              <artifactId>log4j</artifactId>\n              <version>1.2.17</version>\n          </dependency>\n          <dependency>\n              <groupId>org.apache.kafka</groupId>\n              <artifactId>kafka_2.10</artifactId>\n              <version>0.8.1.1</version>\n              <exclusions>\n                  <exclusion>\n                      <groupId>org.apache.zookeeper</groupId>\n                      <artifactId>zookeeper</artifactId>\n                  </exclusion>\n                  <exclusion>\n                      <groupId>log4j</groupId>\n                      <artifactId>log4j</artifactId>\n                  </exclusion>\n              </exclusions>\n          </dependency>\n  \n          <dependency>\n              <groupId>org.apache.storm</groupId>\n              <artifactId>storm-hbase</artifactId>\n              <version>1.0.3</version>\n          </dependency>\n  \n          <dependency>\n              <groupId>org.apache.hadoop</groupId>\n              <artifactId>hadoop-common</artifactId>\n              <version>2.7.3</version>\n          </dependency>\n  \n      </dependencies>\n  \n      <build>\n          <finalName>Storm_test</finalName>\n          <!--这个会输出包中的内容到这个文件夹-->\n          <!--<outputDirectory>/Users/ivanl001/Desktop/everything/</outputDirectory>-->\n          <!--这个会把打好的包放在下面这个文件夹-->\n          <directory>/Users/ivanl001/Desktop/everything/jar/</directory>\n      </build>\n  </project>\n  ```\n\n* 02, 实现IRichBolt，写一个hbaseBolt\n  ```java\n  package im.ivanl001.bigData.Storm.A07_Storm_Hbase;\n\n  import org.apache.hadoop.conf.Configuration;\n  import org.apache.hadoop.hbase.HBaseConfiguration;\n  import org.apache.hadoop.hbase.TableName;\n  import org.apache.hadoop.hbase.client.Connection;\n  import org.apache.hadoop.hbase.client.ConnectionFactory;\n  import org.apache.hadoop.hbase.client.Table;\n  import org.apache.storm.task.OutputCollector;\n  import org.apache.storm.task.TopologyContext;\n  import org.apache.storm.topology.IRichBolt;\n  import org.apache.storm.topology.OutputFieldsDeclarer;\n  import org.apache.storm.tuple.Tuple;\n  \n  import java.io.IOException;\n  import java.util.Map;\n  \n  /**\n   * #author      : ivanl001\n   * #creator     : 2018-11-24 10:49\n   * #description : 发送数据到hbase的bolt节点\n   **/\n  public class WordCountHbaseBolt implements IRichBolt {\n  \n      private Connection connection = null;\n      private Table table = null;\n  \n      public void prepare(Map map, TopologyContext topologyContext, OutputCollector outputCollector) {\n  \n          Configuration configuration = HBaseConfiguration.create();\n          try {\n              connection = ConnectionFactory.createConnection(configuration);\n              table = connection.getTable(TableName.valueOf(\"storm:t1\"));\n          } catch (IOException e) {\n              e.printStackTrace();\n          }\n      }\n  \n      public void execute(Tuple tuple) {\n  \n          String word = tuple.getStringByField(\"word\");\n          Integer count = tuple.getIntegerByField(\"count\");\n  \n          try {\n              table.incrementColumnValue(word.getBytes(), \"f1\".getBytes(), word.getBytes(), count.byteValue());\n              System.out.println(\"+++++++++++\" + word + \"++++++++++++\" + count);\n              //因为storm是流计算，不太可能会停，所以这里table可以不必关闭\n          } catch (IOException e) {\n              e.printStackTrace();\n          }\n  \n      }\n  \n      public void cleanup() {\n  \n      }\n  \n      public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) {\n  \n      }\n  \n      public Map<String, Object> getComponentConfiguration() {\n          return null;\n      }\n  }\n  ```\n  \n* 03, 在app中使用刚才的hbasebolt, 我这里还是使用了二次聚合的方式减少数据倾斜，然后其他几个用到的类这里跟和hbase集成没啥关系，所以就不贴出来了\n  ```java\n  package im.ivanl001.bigData.Storm.A07_Storm_Hbase;\n\n  import org.apache.storm.Config;\n  import org.apache.storm.LocalCluster;\n  import org.apache.storm.StormSubmitter;\n  import org.apache.storm.generated.AlreadyAliveException;\n  import org.apache.storm.generated.AuthorizationException;\n  import org.apache.storm.generated.InvalidTopologyException;\n  import org.apache.storm.topology.TopologyBuilder;\n  import org.apache.storm.tuple.Fields;\n  \n  /**\n   * #author      : ivanl001\n   * #creator     : 2018-11-17 09:09\n   * #description : 这里集成storm和hbase\n   **/\n  public class WordCountApp {\n  \n  \n      public static void main(String[] args) throws InterruptedException, InvalidTopologyException, AuthorizationException, AlreadyAliveException {\n  \n          Config config = new Config();\n          config.setDebug(false);\n          //这个是设置开启几个worker\n          config.setNumWorkers(2);\n  \n          TopologyBuilder builder = new TopologyBuilder();\n  \n          //parallelism_hint是设置并发的数量，比如下面spout设置成2,那么会开两个线程，跑3个任务，这个时候一个线程就必须同时跑两个任务，这其实不太好，所以并发数需要尽量大于任务数\n          builder.setSpout(\"wordCountSpout\", new WordCountSpout(), 2).setNumTasks(2);\n  \n          builder.setBolt(\"wordCountSplitBolt\", new WordCountSplitBolt(), 3).shuffleGrouping(\"wordCountSpout\").setNumTasks(3);\n  \n          //下面的这个hbasebolt其实是逐一递增的，这里的效率其实是不高的，可以先聚合一次，这样子再存到hbase中效率会好很多\n          builder.setBolt(\"wordCountHbaseBolt\", new WordCountHbaseBolt(), 1).shuffleGrouping(\"wordCountSplitBolt\").setNumTasks(1);\n  \n          //这里就不再用默认的类了，而是用hbasebolt类\n          //builder.setBolt(\"wordCountSumBolt\", new WordCountSumBolt(), 4).fieldsGrouping(\"wordCountSplitBolt\", new Fields(\"word\")).setNumTasks(4);\n\n          //这里用本地模式来测试分区模式，消息还是发送到master上的nc好了，懒得改了，一样看\n          LocalCluster localCluster = new LocalCluster();\n          localCluster.submitTopology(\"localWordCountCluster\", config, builder.createTopology());\n          /*Thread.sleep(10000);\n          localCluster.shutdown();*/\n  \n          //集群提交\n          //StormSubmitter.submitTopology(\"wordCountCluster\", config, builder.createTopology());\n      }\n  }\n  ```"
    }
  ]
}