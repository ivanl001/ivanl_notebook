{
  "title": "04-Hive-0102-Hive的基本使用",
  "cells": [
    {
      "type": "markdown",
      "data": "## 1，创建表\n*Hive中表有两种，一种是托管表，默认就是托管表，另外一种是外部表。*\n* 01，托管表在删除表的时候， 数据也会被删除\n* 02, 外部表在删除表的时候， 数据不会被删除\n"
    },
    {
      "type": "markdown",
      "data": "## 2，Hive命令\n*  01, 创建表，默认方式 :\n    CREATE TABLE IF NOT EXISTS t1(id int,name string,age int) COMMENT '这是注释'; \n\n*  02, 创建表，默认方式加强版:  \n    CREATE TABLE IF NOT EXISTS t2(id int,name string,age int) COMMENT '这是注释' ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE;\n* 指定分隔符和不指定分隔符：![IMAGE](quiver-image-url/3786A1A79C05AB2590D9D41D737BB50A.jpg =414x59)\n\n* 03，查看表的具体信息\n    desc t1;\n    desc formatted t1;\n\n* 04, 加载本地文件到hive表，另外需要注意：如果覆盖，之前的数据会全部删除哦， 这个其实是上传本地文件到hive的表目录下\n    > load data local inpath '/root/share/customers.txt' [overwrite] into table t2;//注意：如果文件中有分隔符，创建表的时候也必须要添加分隔符限制\n\n* 05, 加载hdfs上的文件到hive表， 这个其实是把hdfs原目录的文件移动到hive指定的表的目录下\n  > load data inpath '/user/root/customer.txt' into table t3;\n\n* 06, 创建外部表,\n    > CREATE external TABLE IF NOT EXISTS t4(id int,name string,age int) COMMENT '这是注释' ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE;\n    > 加载数据也是和托管表一样，也是上传或者移动数据，但是只是删除数据的时候，文件不会被删除\n\n* 07，复制表，这个和sql是一样的\n  create table t4_copy as select * from t4;//结构和数据都复制\n\n  create table t4_struc_copy like t4;//只复制结构\n  \n* 08，count()查询要转成mr\n\t>select count(*) from t2 ;\n\n* 09, 启用/禁用表，现在貌似不管用\n  \n  ALTER TABLE t2 ENABLE NO_DROP;\t//不允许删除\n\t$hive>ALTER TABLE t2 DISABLE NO_DROP;\t//允许删除\n\n* 10, 创建分区表.\n  分区表,优化手段之一，从目录的层面控制搜索数据的范围。\n  * 首先创建分区表\n    > CREATE TABLE t5(id int,name string,age int) PARTITIONED BY (Year INT, Month INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' ;\n\n  * 然后添加分区，其实就是创建目录\n    > alter table t5 add partition (year=2017, month=12);\n    > alter table t5 add partition (year=2018, month=01) partition(year=2018, month=02);\n  * 然后可以查看分区\n    > show partitions t5;\n \n  * 如果有添加错误的分区或者不需要的分区，可以删除\n    > alter table t5 drop partition (year=2019, month=23);\n\n  * 加载数据到分区，需要指定分区哦\n    > load data local inpath '/root/share/customers.txt' into table t5 partition (year=2018, month=02);\n\n* 11, 桶表\n  *桶表其实就是按文件进行划分，比如说以id值做参考创建三个桶，那么会通过hash值均分到三个文件中*\n\n  * 创建桶表t6\n    create table t6 (id int, name string, age int) clustered by (id) into 3 buckets row format delimited fields terminated by ',';\n\n  * 插入数据, 这里需要注意一点：如果用load加载数据到桶表，是不能实现分桶操作的。可以通过先加载数据到其他表，再复制到桶表中是ok的，我们这里把分区表t5中的数据复制到t6中去, 如果成功后会在t6表中到文件夹中多三个文件，也就是三个桶\n    insert into t6 select id, name, age from t5;//会转mr\n\n  //桶表的数量如何设置?\n\t//评估数据量，保证每个桶的数据量block的2倍大小。一般也就是256M\n\t\n* 12，连接查询\n\t  \n  * 首先创建表\n\t    CREATE TABLE customers(id int,name string,age int) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' ;\n\t    CREATE TABLE orders(id int,orderno string,price float,cid int) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' ;\n  \n  * 然后加载数据到表\n    load data local inpath '/root/share/customers.txt' into table customers;\n    load data local inpath '/root/share/orders.txt' into table orders;\n\n\n  * 通过正常到sql连接语句即可\n    * 内连接\n      > select a.\\*, b.\\* from customers as a, orders as b where a.id = b.cid;\n    * 左外连接\n      > select a.\\*, b.\\* from customers as a left outer join orders as b on a.id = b.cid;\n    * 右外连接\n      > select a.\\*, b.\\* from customers as a right outer join orders as b on a.id = b.cid;\n\n    * 全外连接，注意这个mysql中是没有这个语法的哈，只是hive中有，可以同时查到没有订单的顾客和没有顾客的订单\n      > select a.\\*, b.\\* from customers as a full outer join orders as b on a.id = b.cid;\n* 13， 数据的导入导出\n\n  * 导出表结构+数据。\n    > EXPORT TABLE customers TO '/user/centos/data';\t\t\n\n\n* 14, order:全排序， mr排序\n  > select * from orders order by id desc ;\n\n\n\n\n* 15, sort:map端排序，map端排序,本地有序。\n\n   > select * from orders sort by id desc ;\n   \n   * 可以设置reducer的个数\n    > set mapreduce.job.reduces=0 //这个设置好像无效\n    > set hive.exec.reducers.max=0//这个可以限制\n\n* 16，DISTRIBUTE BY，mapper端的分区操作,需要放在sort之前，因为排序之前需要确定哪些数据进入哪个reducer，要不然怎么排序.....这个作用不是特别懂， 说是类似于mysql的group by， 但好像结果不太对\n  > select cid , orderno, cid from orders distribute by cid sort by cid ;\n\n* 17， cluster by ===>  distribute by cid sort by cid\n  > select cid , orderno, cid from orders cluster by cid ;\n\n\n\n\n\n\n\n"
    },
    {
      "type": "markdown",
      "data": "\n"
    }
  ]
}