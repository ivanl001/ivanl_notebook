{
  "title": "04-Hive-0209-HiveUDF函数",
  "cells": [
    {
      "type": "markdown",
      "data": "*UDF也就是user define function，用户自定义函数,之前已经使用了一小点hive自带的函数，如current_user()等*\n\n---\n*这里顺便先介绍一下打包的两种方式吧：*\n* 1，maven打包， 直接在pom文件中添加：然后通过右侧的maven窗口的package双击即可   \n```xml\n<packaging>jar</packaging>\n<build>\n    <!--这里可以自定义打包的文件的名字哈-->\n    <finalName>imHive</finalName>\n</build>\n```\n* 2, java默认打包\n  > jar cvf IMHive.jar UDF_Add.java\n    * 这个是只打印一个文件\n  > jar cvf IMHive.jar -C class/ .\n    * 这个是打印class文件夹下的所有文件，注意：这里class文件夹是编译后的class文件的文件夹哈，别搞错了，如果打java文件，是没啥作用的\n---"
    },
    {
      "type": "markdown",
      "data": "## 1，一些函数的基本操作\n> show functions;\n  * 显示函数\n  \n> select explode(array(1,2,3));\n  * 这个是一个表生成函数, table generate function, 也就是一样生成多行的函数\n  \n> describe function concat;\n> desc function concat;\n  *  显示某个函数的帮助\n  \n  \n\n## 2，自定义函数步骤\n* 01， 首先maven中添加依赖 \n  ```xml\n  <dependency>\n      <groupId>org.apache.hive</groupId>\n      <artifactId>hive-cli</artifactId>\n      <version>2.1.0</version>\n  </dependency>\n  ```\n  \n* 02，继承org.apache.hadoop.hive.ql.exec.UDF类，然后添加一个evaluate方法，类似如下:\n  ```java\n  package im.ivanl001.bigData.Hive;\n\n  import org.apache.hadoop.hive.ql.exec.Description;\n  import org.apache.hadoop.hive.ql.exec.UDF;\n  \n  /**\n   * #author      : ivanl001\n   * #creator     : 2018-11-06 11:42\n   * #description :\n   **/\n  @Description(name = \"imudf_add\",value = \"value, value, value,xixixixixix\", extended = \"extended, extended, extended...\")\n  public class A02_UDF_Add extends UDF {\n  \n      //这个不是父类的方法，但是需要有这个方法，主要有这种形式即可\n      public int evaluate(int a, int b) {\n          return a + b;\n      }\n  \n      public int evaluate(int a, int b, int c) {\n          return a + b + c;\n      }\n  }\n  ```\n  \n* 03，达成jar包，放到hive安装目录的lib文件夹下\n\n* o4, 在hive命令下: 把刚才的jar包添加到依赖目录中去，如下命令:\n  > add jar /root/hive/lib/imHive.jar;\n\n* 05, 在hive命令下，给刚才的类添加临时函数方法，方便调用（如果不行，请先退出hive重新进入试一下）\n  > create temporary function add as 'im/ivanl001/bigData/Hive/A02_UDF_Add';\n\n* 06, 一切ok之后，可以查看函数，查看函数描述，查看函数扩展描述等等，也可以正常的调用函数了\n  > show functions;\n  > desc function add;\n  > desc function extended add;\n  > select add(1, 3);\n  > select add(1, 3, 2);//这两个方法都有evaluate定义，所以都能正常调用\n"
    },
    {
      "type": "markdown",
      "data": "## 3， 自定义日期时间函数"
    },
    {
      "type": "markdown",
      "data": "### 01. 第一种方式:继承org.apache.hadoop.hive.ql.exec.UDF类\n```java \npackage im.ivanl001.bigData.IMUtils;\n\nimport org.apache.hadoop.hive.ql.exec.UDF;\n\nimport java.text.SimpleDateFormat;\nimport java.util.Date;\n\n/**\n * #author      : ivanl001\n * #creator     : 2018-11-06 15:57\n * #description : 时间处理的自定义函数\n **/\npublic class IMHiveDateUtils extends UDF {\n\n    //这里需要写下面这个函数，也不是父类的方法，但是写上就可以调用，暂时还不明白是什么原理\n    //这里是取出服务器的默认时间，然后转换成字符串，以yyyy-MM-dd HH-mm-ss的格式输出\n    public String evaluate() {\n        Date current = new Date();\n        SimpleDateFormat simpleDateFormat = new SimpleDateFormat(\"yyyy-MM-dd HH-mm-ss\");\n        String resultStr = simpleDateFormat.format(current);\n        return \"currentTime:\" + resultStr;\n    }\n}\n```"
    },
    {
      "type": "markdown",
      "data": "### 02，第二种方式：实现GenericUDF\n*这种方式和第一种方式有些类似，只是它可以在同一个方法里面同时处理多个参数的情况，而不用写多个evaluate函数，这里不再写了，大概知道了*"
    },
    {
      "type": "markdown",
      "data": "## 4，Hive数据倾斜问题\n*教程里面没讲明白，这个好像是处理什么倾斜连接的，也不知道和数据倾斜有啥关系，先放着吧*\n```shell\nhive\n---------------\n\t数据倾斜.\n\t$hive>SET hive.optimize.skewjoin=true;\n\t$hive>SET hive.skewjoin.key=100000;\n\t$hive>SET hive.groupby.skewindata=true;\n\n\nCREATE TABLE mydb.doc(line string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' ;\n\nselect t.word,count(*) from (select explode(split(line,' ')) word from doc ) t group by t.word ;\n\n```\n"
    }
  ]
}