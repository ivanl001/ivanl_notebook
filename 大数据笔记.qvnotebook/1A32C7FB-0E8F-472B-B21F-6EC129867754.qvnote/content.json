{
  "title": "13-Spark-0408-SparkSQL",
  "cells": [
    {
      "type": "markdown",
      "data": "## 通过命令行使用SparkSQL\n\n> val arr = Array(\"1,ivanl001,12\", \"2,ivanl002,22\", \"3,ivanl003,33\")\n\n> val rdd1 = sc.makeRDD(arr)\n\n> val rdd2 = rdd1.map(e => {\n      val arr = e.split(\",\")\n      new Customer(arr(0).toInt, arr(1), arr(2).toInt)\n    })\n    \n> val df = spark.createDataFrame(rdd2)    //相当于创建表\n\n> df.printSchema   //显示表结构\n\n> df.createTempView(\"customer\")  //这里类似创建临时表，后面sql语句用到\n\n> df.show  //显示内 容\n\n> val rdd000 = spark.sql(\"select * from customer\")\n> rdd000.show   //显示刚才sql语句的结果\n\n> val rdd000 = spark.sql(\"select * from customer where age > 15\")\n> rdd000.show\n\n//当然上面的有条件的rdd也还可以再注册成新的小表，再进行其他的操作等等\n\n> val count = spark.sql(\"select count(1) from customer\");\n> count.show  //这里的结果其实都是rdd，通过show显示内容,错了，不是rdd，是dataframe\n\n> val lim = spark.sql(\"select * from customer limit 2\")\n> lim.show\n\n"
    },
    {
      "type": "markdown",
      "data": "## 半SQL\n> spark.sql(\"select id,name from customer\").show\n> df.selectExpr(\"id\", \"name\").show\n\n> df.where(\"name like 'ivanl%'\").show\n\n> df.agg(sum(\"age\")).show            //这个是得到一个dataframe，然后显示\n> df.agg(sum(\"age\"), max(\"age\"), min(\"age\")).show\n> df.map(_.getAs[Int](\"age\")).reduce(_+_)  //算所有年龄的总和\n\n"
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    }
  ]
}