{
  "title": "03-Hadoop-0304-Hadoop安装配置02",
  "cells": [
    {
      "type": "markdown",
      "data": "## 1，namenode和datanode的数据存储位置"
    },
    {
      "type": "markdown",
      "data": "```xml\n\n//namenode和datanode的最终存储位置是由hdfs-site.xml,默认的配置文件在架包中的文件是：hdfs-default.xml中, 他们都是引用变量${hadoop.tmp.dir}，\n//所以由此可见${hadoop.tmp.dir}的位置最终决定我们数据的存储位置，而这个属性是在架包中源文件core-default.xml，或者我们可以更改core-site.xml文件即可实现更改数据存储位置\n\n//---------------------hdfs-default.xml-----------------\n<property>\n  <name>dfs.namenode.name.dir</name>\n  <value>file://${hadoop.tmp.dir}/dfs/name</value>\n  <description>Determines where on the local filesystem the DFS name node\n      should store the name table(fsimage).  If this is a comma-delimited list\n      of directories then the name table is replicated in all of the\n      directories, for redundancy. </description>\n</property>\n\n<property>\n  <name>dfs.datanode.data.dir</name>\n  <!--当然如果有多个磁盘，这里可以以逗号隔开配置多个目录，前面的满了之后会自动从查找后面的-->\n  <value>file://${hadoop.tmp.dir}/dfs/data</value>\n  <description>Determines where on the local filesystem an DFS data node\n  should store its blocks.  If this is a comma-delimited\n  list of directories, then data will be stored in all named\n  directories, typically on different devices. The directories should be tagged\n  with corresponding storage types ([SSD]/[DISK]/[ARCHIVE]/[RAM_DISK]) for HDFS\n  storage policies. The default storage type will be DISK if the directory does\n  not have a storage type tagged explicitly. Directories that do not exist will\n  be created if local filesystem permission allows.\n  </description>\n</property>\n\n\n//---------------------core-default.xml，可更改core-site.xml进行覆盖-----------\n<property>\n  <name>hadoop.tmp.dir</name>\n  <!--更改这个目录即可达到改变namenode和datanode的位置-->\n  <value>/tmp/hadoop-${user.name}</value>\n  <description>A base for other temporary directories.</description>\n</property>\n\n```"
    },
    {
      "type": "markdown",
      "data": "## 2，hdfs命令"
    },
    {
      "type": "markdown",
      "data": "### 2.1， hdfs getconf -confKey\n* 通过这个命令获取某些属性的值，比如说：\n  > hdfs getconf -confKey dfs.namenode.fs-limits.min-block-size\n\n### 2.2, hdfs oiv : offline fsimage viewer\n* hdfs镜像命令，可以通过这个命令查看镜像文件内容(因为是二进制文件，所以不能直接看)\n* 直接查看\n  > hdfs oiv -i fsimage_0000000000000080793\n* 转换成xml文件\n  > hdfs oiv -i fsimage_0000000000000080793 -o /root/test/fsimage01.xml -p XML\n\n### 2.3, hdfs oev :  offline edit viewer\n* 好像是不能直接-i访问，只能-o连带输出，如下\n  > hdfsev -i edits_0000000000000000141-0000000000000000142 -o /root/test/edit01.xml -p XML\n\n### 2.4， hdfs dfsadmin\n* 这个是管理员操作的一个命令集\n* 2.4.1, 安全模式\n  > hdfs dfsadmin -safemode enter\n  > hdfs dfsadmin -safemode leave\n  > hdfs dfsadmin -safemode waite\n* 2.4.2, 设置配额，这个是文件夹个数配额，文件夹本身已经算一个，感觉没啥用，就没练习，后期用到再查看帮助\n  > hdfs dfsadmin -help setQuota\n* 2.4.3, 设置空间配置，同上\n  > hdfs dfsadmin -help setSpaceQuota\n\n```shell\n配额管理(quota)\n-------------------\n\t[目录配额]\n\t计算目录下的所有文件的总个数。如果1，表示空目录。\n\t$>hdfs dfsadmin -setQuota 1 dir1 dir2\t\t//设置目录配额\n\t$>hdfs dfsadmin -clrQuota 1 dir1 dir2\t\t//清除配额管理\n\n\t[空间配额]\n\t计算目录下的所有文件的总大小.包括副本数.\n\t空间配置至少消耗384M的空间大小(目录本身会占用384M的空间)。\n\t$>hdfs dfsadmin -setSpaceQuota 3 data\n\t$>echo -n a > k.txt\n\t$>hdfs dfs -put k.txt data2\n\t$>hdfs dfsadmin -clrSpaceQuota dir1\t\t\t//清除配额管理\n\n```\n\n### 2.5, hdfs dfs\n* 这个dfs命令集是最常用的，其他的就慢慢研究，这里就介绍下创建快照\n  \n  * 为文件夹开启快照功能\n    > hdfs dfsadmin -allowSnapshot /user/root/test\n  * 创建快照\n    > hdfs dfs -createSnapshot /user/root/test\n  * 删除快照,快照名在创建快照的时候会给出\n    > hdfs dfs -deleteSnapshot /user/root/test s20181020-145049.152"
    },
    {
      "type": "markdown",
      "data": "## 3，节点的服役和退役\n*首先先说说明一下，slave文件并不是datanode连接名称节点的文件，这个slave文件只是开启的时候通过调用slave中的host开启这些数据节点，真正的判断是否会连接到名称节点，需要看dfs.hosts和dfs.hosts.exclude两个属性所对应的文件里面的内容，所以正规的服役或者退役的流程必须要先按照下面的配置文件创建好文件，然后配置到hdfs-site.xml中才行，下面的两个文件可以只在namenode节点上有即可*\n```xml\n<property>\n  <name>dfs.hosts</name>\n  <value>/usr/local/hadoop/etc/hadoop/hdfs.hosts.include</value>\n  <description>这里配置文件地址，文件中的host是被允许连接到namenode节点，如果没有配置，那么所有到host地址都可以连接到名称节点上</description>\n</property>\n\n<property>\n  <name>dfs.hosts.exclude</name>\n  <value>/usr/local/hadoop/etc/hadoop/hdfs.hosts.exclude</value>\n  <description>这里配置文件地址，文件中到host是不能连接到名称节点上</description>\n</property>\n```"
    },
    {
      "type": "markdown",
      "data": "### 3.1，退役过程\n* 01，首先需要把需要退役的host添加到hdfs.hosts.exclude中去\n* 02，刷新节点\n  > hdfs dfsadmin -refreshNodes\n* 03, 等待备份文件成功后\n* 04，看webUI界面上提示Decommissioned后，从hdfs.hosts.include删除退役的那个host\n* 05，重新刷新节点，退役完成\n  > hdfs dfsadmin -refreshNodes"
    },
    {
      "type": "markdown",
      "data": "### 3.2, 服役节点，就是退役过程的反过程（整个过程是在已经启动了datanode和nodemanager的情况下进行的）\n* 01，首先把服役节点添加到hdfs.hosts.exclude\n* 02，刷新节点\n* 03，然后添加节点到hdfs.hosts.include\n* 04，刷新节点，看页面提示\n* 05，从hdfs.hosts.exclude中移除要服役到节点\n* 06，刷新节点"
    },
    {
      "type": "markdown",
      "data": "```shell\n节点的服役和退役(hdfs)\n----------------------\n\t[添加新节点]\n\t1.在dfs.include文件中包含新节点名称,该文件在nn的本地目录。\n\t\t[白名单]\n\t\t[s201:/soft/hadoop/etc/dfs.include.txt]\n\t\ts202\n\t\ts203\n\t\ts204\n\t\ts205\n\t2.在hdfs-site.xml文件中添加属性.\n\t\t<property>\n\t\t\t<name>dfs.hosts</name>\n\t\t\t<value>/soft/hadoop/etc/dfs.include.txt</value>\n\t\t</property>\n\n\t3.在nn上刷新节点\n\t\t$>hdfs dfsadmin -refreshNodes\n\n\t4.在slaves文件中添加新节点ip(主机名)\n\t\ts202\n\t\ts203\n\t\ts204\n\t\ts205\t\t//新添加的\n\n\t5.单独启动新的节点中的datanode\n\t\t[s205]\n\t\t$>hadoop-daemon.sh start datanode\n\n\t\t\n\t[退役]\n\t1.添加退役节点的ip到黑名单,不要更新白名单.\n\t\t[/soft/hadoop/etc/dfs.hosts.exclude.txt]\n\t\ts205\n\n\t2.配置hdfs-site.xml\n\t\t<property>\n\t\t\t<name>dfs.hosts.exclude</name>\n\t\t\t<value>/soft/hadoop/etc/dfs.hosts.exclude.txt</value>\n\t\t</property>\n\n\t3.刷新nn节点\n\t\t$>hdfs dfsadmin -refreshNodes\n\n\t4.查看webui,节点状态在decommisstion in progress.\n\n\t5.当所有的要退役的节点都报告为Decommissioned,数据转移工作已经完成。\n\n\t6.从白名单删除节点,并刷新节点\n\t\t[s201:/soft/hadoop/etc/dfs.include.txt]\n\t\t...\n\n\t\t$>hdfs dfsadmin -refreshNodes\n\n\t7.从slaves文件中删除退役节点\n\t\n\t\n节点的服役和退役(yarn)\n----------------------\n\t[添加新节点]\n\t1.在dfs.include文件中包含新节点名称,该文件在nn的本地目录。\n\t\t[白名单]\n\t\t[s201:/soft/hadoop/etc/dfs.include.txt]\n\t\ts202\n\t\ts203\n\t\ts204\n\t\ts205\n\t2.在yarn-site.xml文件中添加属性.\n\t\t<property>\n\t\t\t<name>yarn.resourcemanager.nodes.include-path</name>\n\t\t\t<value>/soft/hadoop/etc/dfs.include.txt</value>\n\t\t</property>\n\n\t3.在nn上刷新节点\n\t\t$>yarn rmadmin -refreshNodes\n\n\t4.在slaves文件中添加新节点ip(主机名)\n\t\ts202\n\t\ts203\n\t\ts204\n\t\ts205\t\t//新添加的\n\n\t5.单独启动新的节点中的nodemananger\n\t\t[s205]\n\t\t$>yarn-daemon.sh start nodemananger\n\n\t\t\n\t[退役]\n\t1.添加退役节点的ip到黑名单,不要更新白名单.\n\t\t[/soft/hadoop/etc/dfs.hosts.exclude.txt]\n\t\ts205\n\n\t2.配置yarn-site.xml\n\t\t<property>\n\t\t\t<name>yarn.resourcemanager.nodes.exclude-path</name>\n\t\t\t<value>/soft/hadoop/etc/dfs.hosts.exclude.txt</value>\n\t\t</property>\n\n\t3.刷新rm节点\n\t\t$>yarn rmadmin -refreshNodes\n\n\t4.查看webui,节点状态在decommisstion in progress.\n\n\t5.当所有的要退役的节点都报告为Decommissioned,数据转移工作已经完成。\n\n\t6.从白名单删除节点,并刷新节点\n\n\t\t$>yarn rmadmin -refreshNodes\n\n\t7.从slaves文件中删除退役节点\n```"
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
    },
    {
      "type": "markdown",
      "data": ""
    }
  ]
}