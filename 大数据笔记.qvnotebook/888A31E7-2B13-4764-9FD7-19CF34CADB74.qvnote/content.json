{
  "title": "13-Spark-0512-SparkStream加和",
  "cells": [
    {
      "type": "markdown",
      "data": "直接上代码8吧：\n\n```java\n\npackage im.ivanl001.bigData.Spark.A12_SparkStream_updateStatusByKey;\n\nimport org.apache.kafka.clients.consumer.ConsumerRecord;\nimport org.apache.kafka.common.serialization.StringDeserializer;\nimport org.apache.spark.SparkConf;\nimport org.apache.spark.api.java.Optional;\nimport org.apache.spark.api.java.function.FlatMapFunction;\nimport org.apache.spark.api.java.function.Function2;\nimport org.apache.spark.api.java.function.PairFunction;\nimport org.apache.spark.streaming.Seconds;\nimport org.apache.spark.streaming.api.java.JavaDStream;\nimport org.apache.spark.streaming.api.java.JavaInputDStream;\nimport org.apache.spark.streaming.api.java.JavaPairDStream;\nimport org.apache.spark.streaming.api.java.JavaStreamingContext;\nimport org.apache.spark.streaming.kafka010.ConsumerStrategies;\nimport org.apache.spark.streaming.kafka010.KafkaUtils;\nimport org.apache.spark.streaming.kafka010.LocationStrategies;\nimport scala.Tuple2;\n\nimport java.util.*;\n\n/**\n * #author      : ivanl001\n * #creator     : 2018-12-01 17:52\n * #description : java版的集成kafka\n **/\npublic class A1201_Stream_updateStatusByKey_java {\n\n    public static void main(String[] args) throws Exception {\n\n\n        //01, 创建配置对象\n        SparkConf conf = new SparkConf();\n        conf.setAppName(\"java_stream\");\n        conf.setMaster(\"local[2]\");\n        //conf.setMaster(\"spark://master:7077\");\n\n        //02, 创建流对象上下文,注意设置间隔时间哦\n        JavaStreamingContext streamingContext = new JavaStreamingContext(conf, Seconds.apply(10));\n\n        // 配置检查点\n        streamingContext.checkpoint(\"file:///Users/ivanl001/Desktop/bigData/checkpoint\");\n\n        //03, 配置kafka的参数\n        Map<String, Object> kafkaParams = new HashMap<>();\n        kafkaParams.put(\"bootstrap.servers\", \"slave01:9092,slave02:9092,slave03:9092\");\n        kafkaParams.put(\"key.deserializer\", StringDeserializer.class);\n        kafkaParams.put(\"value.deserializer\", StringDeserializer.class);\n        kafkaParams.put(\"group.id\", \"use_a_separate_group_id_for_each_stream\");\n        kafkaParams.put(\"auto.offset.reset\", \"latest\");\n        kafkaParams.put(\"enable.auto.commit\", false);\n        //这里可以接受多个主题，不过我们这里就暂时还接受一个就足够了\n        //Collection<String> topics = Arrays.asList(\"topicA\", \"topicB\");\n        Collection<String> topics = Arrays.asList(\"test\");\n\n\n        JavaInputDStream<ConsumerRecord<String, String>> inputDStream =\n                KafkaUtils.createDirectStream(\n                        streamingContext,\n                        LocationStrategies.PreferConsistent(),\n                        ConsumerStrategies.<String, String>Subscribe(topics, kafkaParams)\n                );\n\n\n        //这种方式不是很好处理\n        /*JavaPairDStream<String, String> pairDStream = stream.mapToPair(\n                new PairFunction<ConsumerRecord<String, String>, String, String>() {\n\n                    @Override\n                    public Tuple2<String, String> call(ConsumerRecord<String, String> record) {\n\n                        //这里其实只有一个value，是没有key的哈, 这里的value是输入的一行文字\n                        System.out.println(\"the key is :\" + record.key() + \"-------, the value is : \" + record.value());\n\n                        return new Tuple2<>(record.key(), record.value());\n                    }\n                });*/\n\n        //所以我们用第二种方式\n        JavaDStream<String> words = inputDStream.flatMap(new FlatMapFunction<ConsumerRecord<String, String>, String>() {\n            @Override\n            public Iterator<String> call(ConsumerRecord<String, String> stringStringConsumerRecord) throws Exception {\n                String value = stringStringConsumerRecord.value();\n                return Arrays.asList(value.split(\" \")).iterator();\n            }\n        });\n\n        //05,映射后进行聚合并打印\n        JavaPairDStream<String, Integer> pairDStream = words.mapToPair(new PairFunction<String, String, Integer>() {\n            @Override\n            public Tuple2<String, Integer> call(String s) throws Exception {\n                return new Tuple2<>(s, 1);\n            }\n        });\n\n        //添加这个的还需要配置检查点，我这里在最上面配置，往上看\n        JavaPairDStream updatedPairDStream = pairDStream.updateStateByKey(new Function2<List<Integer>, Optional<Integer>, Optional<Integer>>() {\n            @Override\n            public Optional<Integer> call(List<Integer> v1, Optional<Integer> v2) throws Exception {\n                Integer oldValue = v2.isPresent() ? v2.get() : 0;\n\n                System.out.println(\"值是: \"  + oldValue);\n\n                for (Integer i : v1) {\n                    oldValue += i;\n                }\n                return Optional.of(oldValue);\n            }\n        });\n\n        JavaPairDStream<String, Integer> resultPair = updatedPairDStream.reduceByKey(new Function2<Integer, Integer, Integer>() \n            @Override\n            public Integer call(Integer v1, Integer v2) throws Exception {\n                return v1+v2;\n            }\n        });\n        //这里再打印的时候会打印从开始到现在到总的加和，而不是这个时间段的加和\n        resultPair.print();\n\n        JavaPairDStream<String, Integer> resultPair01 = pairDStream.reduceByKey(new Function2<Integer, Integer, Integer>() {\n            @Override\n            public Integer call(Integer v1, Integer v2) throws Exception {\n                return v1+v2;\n            }\n        });\n        //这里打印的还就是打印每次时间段单独的总和哈，嘿嘿，大概明白了\n        resultPair01.print();\n\n        streamingContext.start();\n        streamingContext.awaitTermination();\n    }\n}\n```"
    }
  ]
}