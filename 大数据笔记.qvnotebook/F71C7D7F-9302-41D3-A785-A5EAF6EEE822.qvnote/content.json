{
  "title": "03-Hadoop-0103-Hadoop安装配置01启动及命令源码分析",
  "cells": [
    {
      "type": "markdown",
      "data": "Hadoop四个模块：\n * 1，common\n * 2, hdfs\n * 3, mapreduce\n * 4, yarn "
    },
    {
      "type": "markdown",
      "data": "## 1，Hadoop安装"
    },
    {
      "type": "markdown",
      "data": "### 1.1，tar -zxvf hadoop-2.7.5.tar.gz #先解压缩\n\n### 1.2，mv hadoop-2.7.5 /usr/local/ #移动被解压的目录到你想要安装到目录下\n\n### 1.3, ln -s hadoop-2.7.5/ hadoop #到刚才移动的目标目录下，创建软链\n\n### 1.4，vim /etc/profile #添加环境变量\n\n```\nexport HADOOP_HOME=/usr/local/hadoop\nexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\n```\n\n### 1.5，hadoop version # 验证是否安装成功\n\n```\n[root@master local]# hadoop version\nHadoop 2.7.5\nSubversion https://shv@git-wip-us.apache.org/repos/asf/hadoop.git -r 18065c2b6806ed4aa6a3187d77cbe21bb3dba075\nCompiled by kshvachk on 2017-12-16T01:06Z\nCompiled with protoc 2.5.0\nFrom source with checksum 9f118f95f47043332d51891e37f736e9\nThis command was run using /usr/local/hadoop-2.7.5/share/hadoop/common/hadoop-common-2.7.5.jar\n```"
    },
    {
      "type": "markdown",
      "data": "## 2，Hadoop不同模式的配置"
    },
    {
      "type": "markdown",
      "data": "\n\n### 2.1， standalone(local)\n\n*单机版模式*\n\n> 默认安装好了之后就是单机版，不需要更改任何文件\n\n### 2.2，pseudodistributed  mode \n\n*伪分布模式, 配置下面四个文件*\n\n> 01，core-site.xml\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n    <property>\n        <name>fs.defaultFS</name>\n        <value>hdfs://localhost/</value>\n    </property>\n</configuration>\n```\n\n> 02，hdfs-site.xml\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n    <property>\n        <name>dfs.replication</name>\n        <value>1</value>\n    </property>\n</configuration>\n```\n\n> 03，mapred-site.xml\n\n```xml\n<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n    <property>\n        <name>mapreduce.framework.name</name>\n        <value>yarn</value>\n    </property>\n</configuration>\n```\n\n> 04，yarn-site.xml\n\n```xml\n<?xml version=\"1.0\"?>\n\n<configuration>\n<!-- Site specific YARN configuration properties -->\n    <property>\n        <name>yarn.resourcemanager.hostname</name>\n        <value>localhost</value>\n    </property>\n    <property>\n        <name>yarn.nodemanager.aux-services</name>\n        <value>mapreduce_shuffle</value>\n    </property>\n</configuration>\n```\n\n#### 启动hadoop\n\n> 01,  启动前先进行格式化，只有第一次启动前格式化，格式化会删除已有hadoop的数据\n\n`hadoop namenode -format`\n\n> 02，启动hadoop，startall.sh，如果报错jdk找不到，到hadoop-env.sh中修改jdk的路径为绝对路径重新启动应该就好了\n\n`start-all.sh`\n\n\n> 03, 查看hadoop的启动状态，jps命令：\n\n`jps如果显示出五个进行说明ok：namenode datenode nodemanager secondarynamenode resourcemanager`\n\n> 04, 网页端查看，端口是50070\n\n`注意：需要先关闭禁用防火墙`\n\n> systemctl status firewalld.service\n> systemctl enable firewalld.service\n> systemctl disable firewalld.service\n> systemctl start firewalld.service\n> systemctl stop firewalld.service\n\n> hadoop的端口\n\n```shell\nNameNode             http port 50070    rpc 8020\nDataNode             http port 50075    rpc 50010\nSecondaryNameNode    http port 50090\n\nNodeManager          \nResourceManager\n```\n\n> hadoop的四个模块\n\n```shell\ncommon\nhdfs    //namenode+datanode+secondarynamenode\nmapred  \nyarn    //resourcemanager+nodemanager\n```\n\n> hadoop启动脚本\n\n```shell\n# 第一种方式\nstart-all.sh\n# stop-all.sh\n\n# 第二种方式\nstart-dfs.sh //这个会启动namenode, datanode, secondarynamenode\nstart-yarn.sh //这个会启动resourcemanager, nodemanager\n```\n\n\n\n### 2.3, fully distributed mode\n\n*完全分布式模式*\n\n> 00, 先更改slaves文件，添加datanode的节点\n\n```shell\n192.168.217.121\n192.168.217.122\n192.168.217.123\n192.168.217.124\n```\n* 为了方便namenode节点和datanode节点相互通信，建议修改一下namenode机器的hosts文件，如下：\n```shell\n127.0.0.1   localhost\n\n# 这一台是namenode节点\n192.168.217.120 master\n\n# 下面三台是datanode节点\n192.168.217.121 slave01\n192.168.217.122 slave02\n192.168.217.123 slave03\n```\n\n\n\n> 01, core-site.xml\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n    <property>\n        <name>fs.defaultFS</name>\n        <!-- 这里是你的namenode的地址 -->\n        <value>hdfs://192.168.217.120/</value>\n    </property>\n</configuration>\n```\n\n> 02, hdfs-site.xml\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n    <property>\n        <name>dfs.replication</name>\n        <!-- 这里是你的datanode的数量 -->\n        <value>3</value>\n    </property>\n</configuration>\n```\n\n> 03, mapred-site.xml\n\n```xml\n<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n    <property>\n        <name>mapreduce.framework.name</name>\n        <value>yarn</value>\n    </property>\n</configuration>\n```\n\n> 04, yarn-site.xml\n\n```xml\n<?xml version=\"1.0\"?>\n\n<configuration>\n<!-- Site specific YARN configuration properties -->\n    <property>\n        <name>yarn.resourcemanager.hostname</name>\n        <!-- 这里是你的资源管理器的地址，其实应该就是namenode节点所在服务器ip -->\n        <value>192.168.217.120</value>\n    </property>\n    <property>\n        <name>yarn.nodemanager.aux-services</name>\n        <value>mapreduce_shuffle</value>\n    </property>\n</configuration>\n```\n"
    },
    {
      "type": "markdown",
      "data": "## 3， 源码分析"
    },
    {
      "type": "markdown",
      "data": "```txt\n脚本分析\n-------------------\n\tsbin/start-all.sh\n\t--------------\n\t\tlibexec/hadoop-config.sh\n\t\tstart-dfs.sh\n\t\tstart-yarn.sh\n\n\tsbin/start-dfs.sh\n\t--------------\n\t\tlibexec/hadoop-config.sh\n\t\tsbin/hadoop-daemons.sh --config .. --hostname .. start namenode ...\n\t\tsbin/hadoop-daemons.sh --config .. --hostname .. start datanode ...\n\t\tsbin/hadoop-daemons.sh --config .. --hostname .. start sescondarynamenode ...\n\t\tsbin/hadoop-daemons.sh --config .. --hostname .. start zkfc ...\t\t\t//\n\t\n\n\tsbin/start-yarn.sh\n\t--------------\t\n\t\tlibexec/yarn-config.sh\n\t\tbin/yarn-daemon.sh start resourcemanager\n\t\tbin/yarn-daemons.sh start nodemanager\n\t\n\n\tsbin/hadoop-daemons.sh\n\t----------------------\n\t\tlibexec/hadoop-config.sh\n\n\t\tslaves\n\n\t\thadoop-daemon.sh\n\n\tsbin/hadoop-daemon.sh\n\t-----------------------\n\t\tlibexec/hadoop-config.sh\n\t\tbin/hdfs ....\n\t\n\n\tsbin/yarn-daemon.sh\n\t-----------------------\n\t\tlibexec/yarn-config.sh\n\t\tbin/yarn\n\n```"
    }
  ]
}