{
  "title": "04-Hive-0201-Hive的使用02",
  "cells": [
    {
      "type": "markdown",
      "data": "---\n注：union在mysql和hive中使用方法一样\n* union查询的使用：\n* mysql> select id, orderno from orders union select id, name from customers;\n* 这个的作用是把不同表查出来的内容竖直的排列在一起， 跟join水平排列在一起是对应的。但是需要注意：union前后的列数需要一直，要不然没法排列在一起。\n---\n\nselect * from TBLS \\G;  😁\\G是行转列，在查看的时候比较方便一些，技能+1；"
    },
    {
      "type": "markdown",
      "data": "## 1，Hive的使用\n### 01，自动分区(动态分区)：\n  *在加入的时候根据加入数据的字段自动分区，而不必把分区数据定死。如下，演示一下动态分区*\n\n  * 首先创建一个有年份和月份的表，用作数据插入的元数据\n    > create table t7 (id int , name string , age int, year string, month string);\n  \n  * 然后插入两条不同年份或月份的数据吧\n    > insert into t7 (id, name, age, year, month) values (1, \"ivanl0000\", 10, 2015, 12);\n  \n    > insert into t7 (id, name, age, year, month) values (2, \"ivanl111111\", 15, 2016, 11);\n  \n  * 然后把上面表中的两条数据插入到我们之前建的分区表t5中去，因为并没有开启动态分区，会报错：\n    > insert into t5 partition (year, month) select id, name, age, year, month from t7;\n  \n  * 上面的插入会报错：\n    > FAILED: SemanticException [Error 10096]: Dynamic partition strict mode requires at least one static partition column. To turn this off set hive.exec.dynamic.partition.mode=nonstrict\n  \n  * 根据提示可以知道需要设置分区模式为非严格模式\n    > set hive.exec.dynamic.partition.mode=nonstrict\n  \n  * 重新插入发现ok了\n  \n### 02，hive的事务\n\n*只有用事务才能支持行级操作，比如更新，删除某一行等等*\n \n \n  * 使用事务必须要先配置如下：\n    ```\n    1.所有事务自动提交。\n    2.只支持orc格式。\n    3.使用bucket表。\n    4.配置hive参数，使其支持事务。\n  \t\n    SET hive.support.concurrency = true;\t\t\t\t\n    SET hive.enforce.bucketing = true;\n    //上面会报错Query returned non-zero code: 1, cause: hive configuration hive.enforce.bucketing does not exists.，先忽略\n    SET hive.exec.dynamic.partition.mode = nonstrict;\t\n    SET hive.txn.manager = org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;\n    SET hive.compactor.initiator.on = true;\n    SET hive.compactor.worker.threads = 1;\n    ```\n  \n  * 显示当前运行的事务\n    > show transactions;\n  \n  * 更新操作(默认使用事务自动提交)\n    > update t2 set name = 'ivanl001' where id = 2;\n    * 其实会报错：FAILED: SemanticException [Error 10297]: Attempt to do update or delete on table imdb.t2 that does not use an AcidOutputFormat or is not bucketed， 因为表不是桶表\n    \n  * 所以我们这里需要重新建立一个桶表,但是跟之前不一样，这里需要额外设置存储格式为orc\n    > create table t8 (id int, name string, age int) clustered by (id) into 3 buckets row format delimited fields terminated by ',' stored as orc;\n\n  * 因为load是没办法完成分桶操作的， 所以还是和之前一样， 直接从t5表中拷贝过来\n    > insert into t8 select id, name, age from t5;\n\n  * 执行更新操作\n    > update t8 set name = \"ivanl001\" where id = 3;\n    * 不过还是会报错： FAILED: SemanticException [Error 10122]: Bucketized tables do not support INSERT INTO: Table: imdb.t8\n    * 最后发现是有一个属性需要在创建表的时候指定，所以我们重新创建表t9, 具体属性如下\n  \n  * 重新创建新表t9:需要设置属性TBLPROPERTIES, 注意：单词别写错了，吃过亏\n    > create table t9 (id int, name string, age int) clustered by (id) into 3 buckets row format delimited fields terminated by ',' stored as orc tblproperties  ('transactional'='true');\n \n  * 然后重新把数据拷贝过来\n    > insert into t9 select id, name, age from t5;\n\n  * 执行更新操作\n    > update t8 set name = \"ivanl001\" where id = 3;\n\n  * 这次就ok啦。嘿嘿😁\n    \n### 03，group by\n*注意：分组聚合的话，select的信息只能是组的信息，而不能是每一条的信息了， 比如说按照用户分组，那么能查出来的肯定是这个用户的比如说订单数量， 订单总金额等等，而不能是具体订单等信息*\n  * select id, name, price , cid from orders group by cid;\n    * 所以这个会直接报错：FAILED: SemanticException [Error 10025]: Line 1:7 Expression not in GROUP BY key 'id'\n    \n  * select count(*), sum(price), cid from orders group by cid;//这样是可以的\n  \n  * select count(*), sum(price) as s, cid from orders group by cid having cid > 30;//分组之后再进行条件筛选"
    },
    {
      "type": "markdown",
      "data": "## 2, Hive函数：wordcount的应用\n\n### 01，hive的wordcount算法\n* 首先创建表，存储需要计算的数据，注意：这里指定的是字段的分隔符\n  > create table word_count (line string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' ;\n\n* 加载数据\n  > load data local inpath '/root/share/word_count.txt' into table word_count;\n\n* 使用内置函数进行计算\n  * 首先把每一行切成数组\n    > select split(line, ' ') from word_count;\n  * 再通过explode函数把数组炸开\n    > select  explode(split(line, ' ')) from word_count;\n  * 之后通过子查询最终确定个数, 这个逻辑有点绕，其实简单来讲，就是把单个单词的集合当作t, 每个单词就是t中的word，我们通过对word分组，算出这组的总数就是了，对吧\n    > select t.word, count(*) from ((select  explode(split(line, ' ')) as word from word_count) as t) group by t.word;\n  \n  * 最后还需要把结果记录下来，只是打印在控制台上是不行的, 这里就是相当于直接创建一个表，把刚才的结果复制进去，也就是create ... as ...就可以了\n    > create table wc_result as select t.word, count(*) from ((select  explode(split(line, ' ')) as word from word_count) as t) group by t.word;"
    },
    {
      "type": "markdown",
      "data": "## 3，Hive的view\n*相当于是复杂查询的快捷方式，把查询语句存到view中去*\n\n* 01，比如说你经常使用到连接查询语句\n  >  select a.\\*, b.\\* from customers as a left outer join orders as b on a.id = b.cid;\n\n* 02，每次都写这么多很麻烦, 所以可以把这语句存入到view中\n  > create view join01_view as select a.\\*, b.\\* from customers as a left outer join orders as b on a.id = b.cid;\n\n* 03, 因为customers 和 orders中都有id字段，会重复，所以这里需要具体指定\n  > create view join01_view as select a.id as aid, a.name, b.id as bid, b.orderno, b.price from customers as a left outer join orders as b on a.id = b.cid;\n\n* 04，显示show tables，可以看到多了一个表，这个表是虚拟表，可以通过查看其信息具体查看\n  > show tables;\n  > desc formatted join01_view;\n\n* 05, 通过操纵这张表来查看刚才我们需要查看的信息,只不过语句就方便多了\n  > select * from join01_view;\n\n* 06, 也可以查某些字段\n  > select aid, price from join01_view;"
    },
    {
      "type": "markdown",
      "data": "## 4，Hive的调优\n\n### Map join: Map端的连接，也就是一小表，一大表的连接\n* 01，设置自动转换，默认就是开启的哈\n  > set hive.auto.convert.join=true\t\t\t//设置自动转换连接,默认开启了。\n\n* 02, 用连接暗示可以进行map端连接，但是这个语句原先也是没有r的，所以这里貌似没有什么参考意义。\n  > select a.\\*,b.\\* from customers a left outer join orders b on a.id = b.cid ;//这里也没有reducer\n\n  > select /*+ mapjoin(customers) */ a.*, b.* from customers a left outer join orders b on a.id = b.cid ;\n  \n### 查看语句的执行流程和整体情况explain\n\n* 01, 在需要查看的语句前添加explain关键字就可以\n  > explain select * from t8;\n  \n  > explain select a.\\*,b.\\* from customers a left outer join orders b on a.id = b.cid;\n\n* 02, 查看更加详尽的步骤信息\n  \n  > explain extended select * from t8;\n\n\n### 对于小数据量数据，可以启用本地模式单机进行处理\n\n* 01, 首先需要设置配置\n  > set mapred.job.tracker=local;\n    * 这一行设置了好像没啥作用，不清楚, 运行mr依然好像是分布式的，然后控制台后台也能查到\n\n  > set hive.exec.mode.local.auto=true;\n    * 这只这一行好像是对的，因为速度快多了，而且分布式的控制台后台也查不到这个进程了，说明是在本地执行的， 当然这个模式小数据测试可以提高速度，生产的话应用意义不大\n    \n### 并行执行 \n  *简单来讲，就是把其中可以拆开的步骤拆开，比如分组，聚合啥的，然后并行执行这些步骤，提高速度*\n  \n  * 01, 设置配置\n    > set hive.exec.parallel=true\n\n  * 02, 执行需要并行的任务就可以了，这里可以自动识别那些是可以并行的，所以很简单\n  \n### 严格模式\n*严格模式禁用了一些查询，比如说不指定分区过滤器查询分区，这样会全盘扫描，数据量比较大等等*\n\n* 01，开启严格模式:\n  > set hive.mapred.mode=strict\n\n* 02, 分区表必须指定分区进行查询\n* 03, order by时必须使用limit子句。\n* 04, 不允许笛卡尔积.\n\n\n### 设置MR的数量\n\n* 01, 设置配置\n  > set hive.exec.reducers.bytes.per.reducer=750000000;\n    * 设置reducer处理的字节数, 这里设置750M\n\n### JVM重用\n\n*大概的意思就是如果设置成1，就是虚拟机的槽位在处理过当前的任务后就会关闭，但是如果设置成其他数值，或者设置成-1，虚拟机在处理当前任务后不会关闭，会等待接受其他任务，重新使用进行处理，知道job结束*\n\n* 01，设置配置\n  > set mapreduce.job.jvm.numtasks=1\t\t//-1没有限制，适用大量小文件。\n\n\n\n\n\n"
    },
    {
      "type": "markdown",
      "data": "\n\n\n\n\n\n\n\n\n\n\n\n\n#ddd"
    }
  ]
}