{
  "title": "13-Spark-0204-RDD变换",
  "cells": [
    {
      "type": "markdown",
      "data": "## 1, RDD变换\n*rdd是不可变的，类似val rdd3 = rdd2.map((_,1))这样的变换都是返回一个新的指针，然后进行下一步操作的*\n\n*返回指向新rdd的指针，在rdd之间创建依赖关系。每个rdd都有计算函数和指向父RDD的指针。*\n\n*RDD变换是lazy的，也就是在没有动作调用之前，其实都是不进行计算的，比如下面的代码：在collect运行之前，都不会打印*\n```scala\n\n//加载文本文件\nval rdd1 = sc.textFile(args(0))\n\n//压扁\nval rdd2 = rdd1.flatMap(line => {\n  //-------------------------------------------------\n  //这里是有一行打印信息的，但是在最后那个collect执行前都不会打印这个信息，因为collect是触发的动作\n  //-------------------------------------------------\n  println(\"ivanl001\" + line)\n  line.split(\" \")\n})\n    \n//映射w => (w,1)\nval rdd3 = rdd2.map((_,1))\nval rdd4 = rdd3.reduceByKey(_ + _)\n\n\nval r = rdd4.collect()\n```\n\n\n---\n---\n---\n\n\n\n### 01，map(func)\n*map(func)和mapPartitions(func)的区别*\n\n  * map仅仅是对每个word进行计算，//对每个元素进行变换，应用变换函数\n  * 而mapPartitions能够识别分区，在某个分区开始或者结束的时候执行某些代码，//对每个分区进行应用变换，输入的Iterator,返回新的迭代器，可以对分区进行函数处理。\n    ```scala\n    //映射w => (w,1)\n    val rdd3 = rdd2.map(word =>{\n      println(\"开始---：\" + word)\n      val res = (word, 1)\n      println(\"结束---:\" + word)\n      res\n    })\n    \n    //这段代码只能在每个单词前或者后添加执行的逻辑，没有办法区分具体的分区\n    ivanl001张丹峰 \n    开始---：张丹峰\n    结束---:张丹峰\n    ivanl001哈哈哈\n    开始---：哈哈哈\n    结束---:哈哈哈\n    ivanl001嘿嘿嘿\n    开始---：嘿嘿嘿\n    结束---:嘿嘿嘿\n    ivanl001厉害啊\n    开始---：厉害啊\n    结束---:厉害啊\n    ivanl001h\n    开始---：h\n    结束---:h\n    ```\n  \n### 02，mapPartitions\n*然后我们可以先用mapPartitions进行分区区分操作，然后再用map映射处理后的数据就可以了*\n  \n  ```scala\n  val rdd2_1 = rdd2.mapPartitions(it => {\n\n  println(\"partition start----\")\n  val buf = ArrayBuffer[String]()\n\n  for (str <- it) {\n    println(\"在mapPartitions中：\" + str+ \"_posfix\")\n    buf += str+ \"_posfix\"\n  }\n\n  println(\"partition end------\")\n\n  buf.iterator\n  })\n  \n  //映射w => (w,1)\n  val rdd3 = rdd2_1.map(word =>{\n    println(\"开始---：\" + word)\n    val res = (word, 1)\n    println(\"结束---:\" + word)\n    res\n  })\n  \n  //可以看到，下面 partition start----只打印一次，因为这里只有一个分区而已，可以在这里设置3个并发\n  //val rdd1 = sc.textFile(\"/Users/ivanl001/Desktop/bigData/input/zhang.txt\", 3)\n\n  partition start----\n  ivanl001张丹峰 \n  在mapPartitions中：张丹峰_posfix\n  ivanl001哈哈哈\n  在mapPartitions中：哈哈哈_posfix\n  ivanl001嘿嘿嘿\n  在mapPartitions中：嘿嘿嘿_posfix\n  ivanl001厉害啊\n  在mapPartitions中：厉害啊_posfix\n  ivanl001h\n  在mapPartitions中：h_posfix\n  partition end------\n  开始---：张丹峰_posfix\n  结束---:张丹峰_posfix\n  开始---：哈哈哈_posfix\n  结束---:哈哈哈_posfix\n  开始---：嘿嘿嘿_posfix\n  结束---:嘿嘿嘿_posfix\n  开始---：厉害啊_posfix\n  结束---:厉害啊_posfix\n  开始---：h_posfix\n  结束---:h_posfix\n  ```\n  \n### 03，mapPartitionsWithIndex(func)\n*mapPartitionsWithIndex和mapPartitions类似,只不过会传递分区索引进来，如下*\n\n  ```scala\n  val rdd2_1 = rdd2.mapPartitionsWithIndex((index,it) => {\n  \n    val t = Thread.currentThread().getName\n    println(\"partition start----\" + t + \"分区索引是:\" + index)\n    val buf = ArrayBuffer[String]()\n    \n    for (str <- it) {\n      println(\"在mapPartitions中：\" + str+ \"_posfix\")\n      buf += str+ \"_posfix\"\n    }\n    \n    println(\"partition end------\")\n    buf.iterator\n  })\n  ```\n  \n### 04,sample(withReplacement, fraction, seed)\n*采样后返回新的rdd，后续使用新的rdd可以减少数据倾斜问题*\n```scala\npackage im.ivanl001.bigData.Spark.A02_Transformations\nimport org.apache.spark.{SparkConf, SparkContext}\nimport scala.collection.mutable.ArrayBuffer\n\n//wordcount主程序\nobject A0203_WordCountApp_Sample {\n\n  def main(args: Array[String]): Unit = {\n    \n    //0, 创建Spark配置对象\n    val conf = new SparkConf()\n\n    //1, 集群模式下下面两行不要\n    conf.setAppName(\"WordCountScala\")\n    //设置master属性\n    //conf.setMaster(\"spark://master:7077\")\n    conf.setMaster(\"local[2]\")//数字是本地模式下开启几个线程模拟多线程\n\n    //2, 通过conf创建sc上下文对象\n    val sc = new SparkContext(conf)\n\n    //3, 加载文本文件\n    //val rdd1 = sc.textFile(args(0))\n    val rdd1 = sc.textFile(\"/Users/ivanl001/Desktop/bigData/input/zhang.txt\", 4)//数字代表分区\n\n    //4, 压扁\n    val rdd2 = rdd1.flatMap(line => {\n//      println(\"ivanl001\" + line)\n      line.split(\" \")\n    })\n\n    //5,教程里面没具体讲，大概就是这里采样一下，给出新的rdd，就差不多可以直接用于后续操作了，因为这个RDD采样后已经处理过了\n    val sampledRdd = rdd2.sample(false, 0.5)\n    sampledRdd.foreach(println(_))\n  }\n}\n```\n\n\n### 05,union(otherDataset)\n*意思就是把两个rdd的内容合并到一个rdd中去*\n\n```scala\npackage im.ivanl001.bigData.Spark.A02_Transformations\nimport org.apache.spark.{SparkConf, SparkContext}\n\n//wordcount主程序\nobject A0204_WordCountApp_Union {\n\n  def main(args: Array[String]): Unit = {\n\n    val conf = new SparkConf()\n    conf.setAppName(\"WordCountScala\")\n    conf.setMaster(\"local[4]\") ;\n    val sc = new SparkContext(conf)\n\n    val rdd1 = sc.textFile(\"/Users/ivanl001/Desktop/bigData/input/log.txt\",2)\n\n    //所有error\n    //这里是单独的带有error的行\n    val errorRDD = rdd1.filter(_.toLowerCase.contains(\"error\"))\n    errorRDD.foreach(line => {\n      println(line)\n      println(\"-----\")\n    })\n\n    //所有warn行\n    //这里是单独的带有warn的行\n    val warnRDD = rdd1.filter(_.toLowerCase.contains(\"warn\"))\n    warnRDD.foreach(line => {\n      println(line)\n      println(\"++++\")\n    })\n    \n    //下面可以通过union的方式把所有的行放在一起然后放在一个RDD中\n    val allRDD = errorRDD.union(warnRDD)\n    println(\"==================\")\n    allRDD.collect().foreach(println)\n  }\n}\n```\n\n\n### 06, intersection(otherDataset)\n*返回两个rdd中有交集的部分*\n\n```scala\npackage im.ivanl001.bigData.Spark.A02_Transformations\nimport org.apache.spark.{SparkConf, SparkContext}\n\n//wordcount主程序\nobject A0205_WordCountApp_Intersection {\n\n  def main(args: Array[String]): Unit = {\n\n    val conf = new SparkConf()\n    conf.setAppName(\"WordCountScala\")\n    conf.setMaster(\"local[4]\") ;\n    val sc = new SparkContext(conf)\n    val rdd1 = sc.textFile(\"/Users/ivanl001/Desktop/bigData/input/log.txt\",4)\n    //所有error\n    val errorRDD = rdd1.filter(_.toLowerCase.contains(\"error\"))\n\n    //所有warn行\n    val warnRDD = rdd1.filter(_.toLowerCase.contains(\"warn\"))\n\n    //这里可以通过intersection返回两个rdd中相同的部分，也就是有交集的地方\n    val intersecRDD = errorRDD.intersection(warnRDD)\n    intersecRDD.collect().foreach(println)\n  }\n}\n```\n\n\n\n### 07,distinct([numTasks])) \n*这个可以取出某个rdd中重复的内容*\n\n```scala\npackage im.ivanl001.bigData.Spark.A02_Transformations\n\nimport org.apache.spark.{SparkConf, SparkContext}\n\n//wordcount主程序\nobject A0206_WordCountApp_distinct {\n\n  def main(args: Array[String]): Unit = {\n\n    val conf = new SparkConf()\n    conf.setAppName(\"WordCountScala\")\n    conf.setMaster(\"local[4]\") ;\n    val sc = new SparkContext(conf)\n    val rdd1 = sc.textFile(\"/Users/ivanl001/Desktop/bigData/input/log.txt\",4)\n    //所有error\n    //val errorRDD = rdd1.filter(_.toLowerCase.contains(\"error\"))\n\n    //所有warn行\n    val warnRDD = rdd1.filter(_.toLowerCase.contains(\"warn\"))\n    //这里是可以打印出相同的那些行的\n    warnRDD.foreach(println)\n\n    println(\"--------------------------------------\")\n    \n    //这里可以通过distinct去掉某个rdd中重复的部分\n    //下面经过去重之后就不会打印出相同的行了，相同的行只能打印出一行哈\n    val distinctedWarnRDD = warnRDD.distinct()\n    distinctedWarnRDD.foreach(println)\n  }\n}\n```\n\n### 08, groupByKey([numTasks])\n*根据key分组,比较好理解，看代码*\n> (K, V) => (K, Iterable<V>)\n```scala\npackage im.ivanl001.bigData.Spark.A02_Transformations\nimport org.apache.spark.{SparkConf, SparkContext}\n\n//wordcount主程序\nobject A0207_WordCountApp_groupByKey {\n\n  def main(args: Array[String]): Unit = {\n    val conf = new SparkConf()\n    conf.setAppName(\"WordCountScala\")\n    conf.setMaster(\"local[4]\") ;\n    val sc = new SparkContext(conf)\n    val rdd1 = sc.textFile(\"/Users/ivanl001/Desktop/bigData/input/students.txt\",1)\n\n    //这里把原先的一行数据映射成(省份,行数据)格式\n    val rdd2 = rdd1.map(line => {\n      val key = line.split(\" \")(2)\n      (key, line)\n    })\n\n    //因为groupByKey把结果转换成(K, V) => (K, Iterable)这种，所以按照如下方式进行打印\n    val rdd3 = rdd2.groupByKey()\n    rdd3.foreach(e => {\n      val key = e._1\n      val iterable = e._2\n      for (res <- iterable) {\n        println(key + \":\" + res)\n      }\n      println(\"=====================\")\n    })\n  }\n}\n//结果如下:这个是根据key分组后的结果，注意看代码中分组后的返回值rdd3中的value是iterable\n/*\nbeijing:4 ivanl004 beijing\nbeijing:8 ivanl008 beijing\n=====================\nshanghai:3 ivanl003 shanghai\nshanghai:7 ivanl007 shanghai\n=====================\nhenan:1 ivanl001 henan\nhenan:2 ivanl002 henan\nhenan:6 ivanl006 henan\n=====================\nshangdong:5 ivanl005 shangdong\n=====================\n*/\n```\n\n### 09, aggregateByKey(zeroValue)(seqOp, combOp, [numTasks])\n\n\n    \n### 10, sortByKey([ascending], [numTasks])\n*很明显，就是排序, 代码如下:*\n```scala\npackage im.ivanl001.bigData.Spark.A02_Transformations\n\nimport org.apache.spark.{SparkConf, SparkContext}\n\n//wordcount主程序\nobject A0208_WordCountApp_sortByKey {\n\n  def main(args: Array[String]): Unit = {\n\n    val conf = new SparkConf()\n    conf.setAppName(\"WordCountScala\")\n    conf.setMaster(\"local[4]\") ;\n    val sc = new SparkContext(conf)\n    val rdd1 = sc.textFile(\"/Users/ivanl001/Desktop/bigData/input/students.txt\",1)\n\n    //这里把原先的一行数据映射成(省份,行数据)格式\n    val rdd2 = rdd1.map(line => {\n      val key = line.split(\" \")(2)\n      (key, line)\n    })\n\n    //因为groupByKey把结果转换成(K, V) => (K, Iterable)这种，所以按照如下方式进行打印\n    val rdd3 = rdd2.sortByKey()\n    rdd3.foreach(e => {\n      val key = e._1\n      val value = e._2\n\n      println(key + \"========\" + value)\n    })\n  }\n}\n//结果如下:这个是根据key排序后的结果\n/*\nbeijing========4 ivanl004 beijing\nbeijing========8 ivanl008 beijing\nhenan========1 ivanl001 henan\nhenan========2 ivanl002 henan\nhenan========6 ivanl006 henan\nshangdong========5 ivanl005 shangdong\nshanghai========3 ivanl003 shanghai\nshanghai========7 ivanl007 shanghai\n*/\n```\n\n\n### 11, join(otherDataset, [numTasks])\n*上面05中已经讲过union，union就是把两个rdd的内容合并到一个rdd中去，是纵向的，join不一样，join是横向的*\n*方式大概是: (K, V) and (K, W)  =>  (K, (V, W)),看起来和groupByKey有点像对吧，但是前者是针对多张表进行的join，而group是在单张表中进行分组的哈*\n```scala\npackage im.ivanl001.bigData.Spark.A02_Transformations\nimport org.apache.spark.{SparkConf, SparkContext}\n\n//wordcount主程序\nobject A0209_WordCountApp_join {\n\n  def main(args: Array[String]): Unit = {\n\n    val conf = new SparkConf()\n    conf.setAppName(\"WordCountScala\")\n    conf.setMaster(\"local[4]\") ;\n    val sc = new SparkContext(conf)\n    \n    val customerRdd = sc.textFile(\"/Users/ivanl001/Desktop/bigData/input/customer.txt\",1)\n    val ordersRdd = sc.textFile(\"/Users/ivanl001/Desktop/bigData/input/orders.txt\",1)\n    \n    val customerRdd2 = customerRdd.map(line => (line.split(\" \")(0),line))\n    val orderRdd2 = ordersRdd.map(line => (line.split(\" \")(0),line))\n\n    val rdd = customerRdd2.join(orderRdd2)\n    rdd.foreach(e => {\n      val key = e._1\n      val value = e._2\n\n      println(key + \":::::\" + value)\n    })\n  }\n}\n//结果如下\n/*\n2:::::(2 ivanl002,2 ae)\n3:::::(3 ivanl003,3 西红柿)\n3:::::(3 ivanl003,3 苹果)\n1:::::(1 ivanl001,1 eos5dMark4)\n1:::::(1 ivanl001,1 iphone)\n1:::::(1 ivanl001,1 finalCut)\n*/\n```\n\n### 12, cogroup(otherDataset, [numTasks])\n*(K, V) and (K, W)  =>  (K, (Iterable<V>, Iterable<W>))*\n\n```scala\n/*\n意思大概知道了，这里就不再写练习了\n比如说有很多学校，一张表记录每个学校的老师，另外一张表记录每个学校的学生，想要把每个学校的老师学生放在一起就可以用这个cogroup，因为一个学校会对应多个老师,一个学校也会对应多个学生\n*/\n\npackage im.ivanl001.bigData.Spark.A02_Transformations\n\nimport org.apache.spark.{SparkConf, SparkContext}\n\n//wordcount主程序\nobject A0210_WordCountApp_cogroup {\n\n  def main(args: Array[String]): Unit = {\n\n    val conf = new SparkConf()\n    conf.setAppName(\"WordCountScala\")\n    conf.setMaster(\"local[4]\") ;\n    val sc = new SparkContext(conf)\n\n    //表中的数据并不符合实际情况，不过这里就是为了演示一下协分组的，没必要深究\n    val customerRdd = sc.textFile(\"/Users/ivanl001/Desktop/bigData/input/customer01.txt\",1)\n    val ordersRdd = sc.textFile(\"/Users/ivanl001/Desktop/bigData/input/orders01.txt\",1)\n\n    val customerRdd2 = customerRdd.map(line => (line.split(\" \")(0),line))\n    val orderRdd2 = ordersRdd.map(line => (line.split(\" \")(0),line))\n\n    val rdd = customerRdd2.cogroup(orderRdd2)\n\n    rdd.foreach(e => {\n\n      println(\"key:\" + e._1)\n\n      val val01 = e._2._1\n      val val02 = e._2._2\n\n      for (value <- val01) {\n        print(value + \"  \")\n      }\n      println()\n      for (value <- val02) {\n        print(value + \"  \")\n      }\n      println()\n    })\n  }\n}\n//结果是:可以重点看下key=1的时候，这个时候不论是顾客还是商品都是对应多个，这个也就是故意做的数据，为了演示一下协分组而已，不必在意逻辑性\n/*\nkey:2\n2 ivanl002  \n2 ae  \nkey:3\n3 ivanl003  \n3 西红柿  3 苹果  \nkey:1\n1 ivanl001  1 ivanl0001  1 ivanl00001  \n1 eos5dMark4  1 iphone  1 finalCut  \n*/\n```\n\n### 13，cartesian(otherDataset)\n*笛卡尔积，就是把两张表的内容进行笛卡尔积获取，应该用的也不多*\n```scala\npackage im.ivanl001.bigData.Spark.A02_Transformations\nimport org.apache.spark.{SparkConf, SparkContext}\n\n//wordcount主程序\nobject A0211_WordCountApp_cartesian {\n\n  def main(args: Array[String]): Unit = {\n\n    val conf = new SparkConf()\n    conf.setAppName(\"WordCountScala\")\n    conf.setMaster(\"local[4]\") ;\n    val sc = new SparkContext(conf)\n\n    //这里用暴力破解密码的案例演示一下笛卡尔积，只知道几个用户名和几个密码，不知道怎么对应，就用笛卡尔积试试\n    val usernameRdd = sc.textFile(\"/Users/ivanl001/Desktop/bigData/input/username.txt\",1)\n    val passwordRdd = sc.textFile(\"/Users/ivanl001/Desktop/bigData/input/password.txt\",1)\n\n    val result = usernameRdd.cartesian(passwordRdd)\n    result.foreach(println(_))\n  }\n}\n//一共三个用户名，三个密码，结果会是九个，如下:\n/*\n(ivanl001,13579)\n(ivanl001,24680)\n(ivanl001,12345)\n(ivanl002,13579)\n(ivanl002,24680)\n(ivanl002,12345)\n(ivanl003,13579)\n(ivanl003,24680)\n(ivanl003,12345)\n*/\n/*\nval usernameRdd = sc.parallelize(Array(\"ivanl001\", \"ivanl002\", \"ivanl003\"))\nval passwordRdd = sc.parallelize(Array(\"123\", \"456\", \"789\"))\n创建rdd的时候，也可以通过上面的方式\n*/\n```\n### 13，pipe(command, [envVars])\n*这个可以执行shell脚本命令，把结果包装成rdd返回*\n```scala\npackage im.ivanl001.bigData.Spark.A02_Transformations\nimport org.apache.spark.{SparkConf, SparkContext}\n\n//wordcount主程序\nobject A0212_WordCountApp_pipe {\n\n  def main(args: Array[String]): Unit = {\n\n    val conf = new SparkConf()\n    conf.setAppName(\"WordCountScala\")\n    conf.setMaster(\"local[4]\") ;\n    val sc = new SparkContext(conf)\n\n    val rdd01 = sc.parallelize(\"/Users/ivanl001/Desktop/\")\n    //这个在linux下可以，在mac或者wins下好像不好使\n    //val rdd02 = rdd01.pipe(\"ls\")\n    val rdd02 = rdd01.pipe(\"ls /Users/ivanl001/Desktop/\")\n    rdd02.foreach(println(_))\n  }\n}\n```\n\n### 14, coalesce(numPartitions)和repartition(numPartitions)\n*前者是减少分区，后者是重新分区，可以减少分区，也能增加分区*\n\n```scala\npackage im.ivanl001.bigData.Spark.A02_Transformations\nimport org.apache.spark.{SparkConf, SparkContext}\n\n//wordcount主程序\nobject A0213_WordCountApp_coalesce {\n\n  def main(args: Array[String]): Unit = {\n\n    //0, 创建Spark配置对象\n    val conf = new SparkConf()\n\n    //1, 集群模式下下面两行不要\n    conf.setAppName(\"WordCountScala\")\n    //设置master属性\n    //conf.setMaster(\"spark://master:7077\")\n    conf.setMaster(\"local[2]\")//数字是本地模式下开启几个线程模拟多线程\n\n    //2, 通过conf创建sc上下文对象\n    val sc = new SparkContext(conf)\n\n    //3, 加载文本文件\n    //val rdd1 = sc.textFile(args(0))\n    val rdd1 = sc.textFile(\"/Users/ivanl001/Desktop/bigData/input/zhang.txt\", 5)//数字代表分区\n    println(\"rdd1 partition:\" + rdd1.partitions.length)\n\n    //这个变换可以减少分区，返回一个rdd，通过新的rdd进行相关操作即可\n    //但是要注意：这里只能减少分区，不能增加，如果需要增加用repartition\n    val rdd01 = rdd1.coalesce(3)\n\n    //4, 压扁\n    val rdd2 = rdd01.flatMap(_.split(\" \"))\n    println(\"rdd2 partition:\" + rdd01.partitions.length)\n\n    //通过repartition可以增加分区，其实用这个也能减少分区\n    val rdd02 = rdd2.repartition(7)\n\n    //5,映射\n    val rdd3 = rdd02.map((_,1))\n    println(\"rdd3 partition:\" + rdd3.partitions.length)\n  }\n}\n```\n\n### 15, repartitionAndSortWithinPartitions(partitioner)\n*重新分区并在分区内进行排序*\n"
    },
    {
      "type": "markdown",
      "data": ""
    }
  ]
}