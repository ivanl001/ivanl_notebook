{
  "title": "13-Spark-0501-SparkSQL的使用和集成hive等",
  "cells": [
    {
      "type": "markdown",
      "data": "* json文件的读取与写入\n* mysql数据库的读取与写入\n* hive数据库的集成\n```xml\n<!--这个依赖下面几个都需要引入-->\n<dependency>\n    <groupId>org.apache.spark</groupId>\n    <artifactId>spark-sql_2.11</artifactId>\n    <version>2.1.0</version>\n</dependency>\n```"
    },
    {
      "type": "markdown",
      "data": "## 1，json文件的读取与写入\n```scala\npackage im.ivanl001.bigData.Spark.A08_SparkSQL\n\nimport org.apache.spark.sql.{Row, SaveMode, SparkSession}\nimport org.apache.spark.{SparkConf, SparkContext}\n\nobject A0801_SparkSQL_json {\n\n  def main(args: Array[String]): Unit = {\n\n    //0，首先创建session类，用于后面读取或者写入数据\n    val spark = SparkSession\n      .builder()\n      .appName(\"Spark SQL basic example\")\n      .config(\"spark.some.config.option\", \"some-value\")\n      .getOrCreate()\n\n    //1，读取json文件数据\n    val df = spark.read.json(\"/Users/ivanl001/Desktop/bigData/sparkSql/customers.json\")\n    println(df.show())\n\n    //2, 这个相当于创建表，然后显示出来\n    df.createOrReplaceTempView(\"customers\")\n    df.show()\n\n    //3，条件查询\n    //---------查询年纪大于10的两种方式-----------\n    //----------方式一---------------------\n    println(\"方式一\")\n    val df01 = df.where(\"age > 10\")\n    df01.show()\n\n    //4, sql方式查询，查询和3一样的要求内容：age > 10\n    //----------方式二------------------\n    println(\"方式二\")\n    val df02 = spark.sql(\"select * from customers where age > 10\")\n    df02.show()\n    for (row <- df02) {\n      println(\"ivanl001\" + row)\n    }\n\n    //5，聚合查询\n    //--------聚合查询总数----------\n    val count = spark.sql(\"select count(1) from customers\")\n    count.show()\n\n    //6, 转换成RDD进行其他操作\n    //转成rdd进行处理\n    val rdd = df01.rdd\n    rdd.foreach(row => {\n      val age = row.getLong(0)\n      val id = row.getLong(1)\n      val name = row.getString(2)\n      println(\"age:\" + age + \",name:\" + name + \",num:\" + id)\n    })\n\n    //df01.write.json(\"/Users/ivanl001/Desktop/bigData/sparkSql/customers01.json\")\n\n    //7, 保存\n    //设置保存模式\n    df01.write.mode(SaveMode.Append).json(\"/Users/ivanl001/Desktop/bigData/sparkSql/customers01.json\")\n  }\n}\n```"
    },
    {
      "type": "markdown",
      "data": "## 2，mysql数据库的读取与写入\n* 01，需要先引入依赖\n  ```xml\n  <dependency>\n      <groupId>mysql</groupId>\n      <artifactId>mysql-connector-java</artifactId>\n      <version>5.1.17</version>\n  </dependency>\n  ```\n* 02, 代码如下 \n```java\npackage im.ivanl001.bigData.Spark.A08_SparkSQL;\nimport org.apache.spark.sql.Column;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\nimport javax.xml.crypto.Data;\nimport java.util.Properties;\n/**\n * #author      : ivanl001\n * #creator     : 2018-11-30 20:54\n * #description :\n **/\npublic class A0802_SparkSQL_JDBC_java {\n\n   public static void main(String[] args) {\n\n      //0，创建session对象，方便后面读取或者存入数据库\n      SparkSession spark = SparkSession.builder().appName(\"jdbc\").config(\"spark.master\", \"local\").getOrCreate();\n      String url = \"jdbc:mysql://localhost:3306/test\";\n      String table = \"customers\";\n\n      //1，查询数据库\n      Dataset<Row> df = spark.read().format(\"jdbc\").option(\"url\", url).option(\"dbtable\", table).option(\"user\", \"root\").option(\"password\", \"ivanl48265\").option(\"driver\", \"com.mysql.jdbc.Driver\").load();\n      df.show();\n\n\n      //2, 投影查询\n      Dataset<Row> df01 = df.select(new Column(\"name\"), new Column(\"id\"));\n      df01.show();\n      df01.where(\"id > 2\").show();\n      //这里可以去重，不过我门这里没有重复的，所以没啥用\n      df01.distinct().show();\n\n      //3, 写入到表中\n      Properties prop = new Properties();\n      prop.put(\"user\", \"root\");\n      prop.put(\"password\", \"ivanl48265\");\n      prop.put(\"driver\", \"com.mysql.jdbc.Driver\");\n      df.write().jdbc(url, \"new\", prop);\n   }\n}\n```\n"
    },
    {
      "type": "markdown",
      "data": "## 3，hive数据库的集成\n* 01，复制hive和hdfs的如下配置文件到spark的conf目录下\n*Configuration of Hive is done by placing your hive-site.xml, core-site.xml (for security configuration), and hdfs-site.xml (for HDFS configuration) file in conf/.*\n  ```\n  hive-site.xml\n  core-site.xml (for security configuration)\n  hdfs-site.xml (for HDFS configuration)\n  ```\n* 02，复制mysql-connector-java-5.1.17.jar驱动程序到spark到jar目录下\n* 03，spark-shell --master local[4]启动程序\n* 04，输入如下命令检查是否集成成功\n  > sql(\"show databases\").show\n* 05, 如果显示出数据库列表，集成成功\n"
    },
    {
      "type": "markdown",
      "data": "## 4, spark-hive的使用\n* 01，首先在hdfs上传一个用于创建表的数据文件，内容如下:\n  ```texst\n  1,ivanl001,10\n  2,ivanl002,20\n  3,ivanl003,30\n  4,ivanl004,40\n  5,ivanl005,50\n  6,ivanl006,60\n  ```\n  \n* 02, 在hive上创建表\n  > spark.sql(\"create table imdb.hiveSpark(id int, name string, age int) row format delimited fields terminated by ',' lines terminated by '\\n' stored as textfile\")\n\n* 03, 加载数据\n  > spark.sql(\"load data inpath '/user/root/hiveData.dat' into table imdb.hiveSpark\")\n\n* 04, 查看数据\n  > sql(\"select * from imdb.hiveSpark\").show\n* 05, 聚合\n  > sql(\"select sum(age) from imdb.hiveSpark\").show"
    },
    {
      "type": "markdown",
      "data": "## 5, spark-hive代码流程\n\n* 01,同样拷贝三个配置文件\n\n* 02,因为hive是单机版的，所以如果需要连接需要开启thrift服务器，spark自身已经集成了thrift服务器，可以直接开启spark的thrift服务器，连接spark，通过spark连接hive更为方便，在sbin目录下 \n  > start-thriftserver.sh --master  spark://master:7077\n\n\n* 03,如果启动成功，10000端口会被监听\n  > netstat -anop | grep 10000\n\n* 04,通过spark的bin目录下的beeline连接\n  > eeline -u jdbc:hive2://localhost:10000 -d org.apache.hive.jdbc.HiveDriver\n\n* 05,进入命令行后执行查询或者其他任务即可\n  > show databases;\n \n* 06,thrift启动后，maven中添加如下依赖\n```xml\n<!--spark核心架包-->\n<!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core -->\n<dependency>\n    <groupId>org.apache.spark</groupId>\n    <artifactId>spark-core_2.11</artifactId>\n    <version>2.1.1</version>\n</dependency>\n\n<!--sparksql使用的时候需要两个架包-->\n<dependency>\n    <groupId>org.apache.spark</groupId>\n    <artifactId>spark-sql_2.11</artifactId>\n    <version>2.1.0</version>\n</dependency>\n\n<dependency>\n    <groupId>mysql</groupId>\n    <artifactId>mysql-connector-java</artifactId>\n    <version>5.1.17</version>\n</dependency>\n\n<!--sparksql和hive集成需要的架包-->\n<dependency>\n    <groupId>org.apache.hive</groupId>\n    <artifactId>hive-jdbc</artifactId>\n    <version>2.1.0</version>\n</dependency>\n```\n\n* 07, 编写代码，连接10000端口，进行指定的hive操作即可\n```java\npackage im.ivanl001.bigData.Spark.A09_SparkSQLAndHive;\n\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\n\nimport java.sql.Connection;\nimport java.sql.DriverManager;\nimport java.sql.ResultSet;\nimport java.sql.Statement;\n\n/**\n * #author      : ivanl001\n * #creator     : 2018-12-01 12:36\n * #description :\n **/\npublic class A0901_Spark_Hive_java {\n\n    public static void main(String[] args) throws Exception {\n\n        /*//0，创建session对象，方便后面读取或者存入数据库\n        SparkSession spark = SparkSession.builder().appName(\"jdbc\").config(\"spark.master\", \"local\").getOrCreate();\n\n        //00，尝试创建表\n        spark.sql(\"create table imdb.table(id int)\");\n\n        //1，获取\n        *//*Dataset<Row> df00 = spark.sql(\"select * from imdb.hiveSpark\");\n        df00.show();*/\n\n        //因为如果集成hive，需要开启spark下的beeline和thrift服务器，所以我们这里直接使用10000端口进行连接即可\n        //thrift的连接方式也是rpc的，类似mysql，具体如下\n        Class.forName(\"org.apache.hive.jdbc.HiveDriver\");\n        Connection connection = DriverManager.getConnection(\"jdbc:hive2://master:10000\");\n        Statement st = connection.createStatement();\n\n        ResultSet resultSet = st.executeQuery(\"select * from imdb.hiveSpark\");\n        while (resultSet.next()) {\n            int id = resultSet.getInt(\"id\");\n            String name = resultSet.getString(\"name\");\n            int age = resultSet.getInt(\"age\");\n            System.out.println(\"id:\" + id + \"----------aget:\" + age + \"----------name:\" + name);\n        }\n\n        ResultSet resultSet1 = st.executeQuery(\"select count(1) from imdb.hiveSpark\");\n        while (resultSet1.next()) {\n            System.out.printf(\"个数是：\"+ resultSet1.getInt(\"1\"));\n        }\n    }\n}\n```\n"
    }
  ]
}