{
  "title": "13-Spark-0506-SparkStream模块",
  "cells": [
    {
      "type": "markdown",
      "data": "## 1，Stream模块的使用\n* 01, 添加maven依赖\n  ```xml\n  <!--spark-stream计算需要的架包-->\n  <dependency>\n      <groupId>org.apache.spark</groupId>\n      <artifactId>spark-streaming_2.11</artifactId>\n      <version>2.1.0</version>\n  </dependency>\n\n  <dependency>\n      <groupId>org.apache.spark</groupId>\n      <artifactId>spark-streaming-kafka-0-10_2.11</artifactId>\n      <version>2.1.0</version>\n  </dependency>\n  ```\n* 02, 编写代码\n  *scala代码*\n  ```scala\n  package im.ivanl001.bigData.Spark.A10_SparkStream\n\n  import org.apache.spark.SparkConf\n  import org.apache.spark.streaming.{Seconds, StreamingContext}\n  \n  object A1001_SparkStreamDemo {\n  \n    def main(args: Array[String]): Unit = {\n  \n      //01, 创建Spark配置对象\n      /*val conf = new SparkConf()\n      //集群模式下下面两行不要\n      conf.setAppName(\"WordCountScala\")\n      //设置master属性\n      //conf.setMaster(\"spark://master:7077\")\n      conf.setMaster(\"local[2]\")//数字是本地模式下开启几个线程模拟多线程*/\n  \n  \n      //01, 下面我们用第二种方式创建配置对象,有一点需要注意：流处理是在分线程执行，所以这里必须设置大于1条线程才行\n      //val conf = new SparkConf().setAppName(\"streamDemo\").setMaster(\"local[2]\")\n      //分布式下的设置\n      val conf = new SparkConf().setAppName(\"streamDemo\").setMaster(\"spark://master:7077\")\n  \n      //02, 创建流对象上下文,注意设置间隔时间哦\n      val streamContext = new StreamingContext(conf, Seconds(10))//分布式下设置成10秒吧\n  \n      //03,创建nc文本流用于接受数据\n      // Create a DStream that will connect to hostname:port, like localhost:9999\n      val lines = streamContext.socketTextStream(\"master\", 9999)\n  \n      //04, 切割数据\n      val words = lines.flatMap(_.split(\" \"))\n  \n      //05，映射后进行聚合并打印\n      // Count each word in each batch\n      val pairs = words.map(word => (word, 1))\n      val wordCounts = pairs.reduceByKey(_ + _)\n  \n      // Print the first ten elements of each RDD generated in this DStream to the console\n      wordCounts.print()\n  \n      //06, 正式开始程序等\n      streamContext.start()             // Start the computation\n      streamContext.awaitTermination()  // Wait for the computation to terminate\n    }\n  }\n  ```\n  *java代码*\n  ```java\n  package im.ivanl001.bigData.Spark.A10_SparkStream;\n\n  import org.apache.spark.SparkConf;\n  import org.apache.spark.api.java.function.FlatMapFunction;\n  import org.apache.spark.api.java.function.Function2;\n  import org.apache.spark.api.java.function.PairFunction;\n  import org.apache.spark.streaming.Seconds;\n  import org.apache.spark.streaming.api.java.JavaDStream;\n  import org.apache.spark.streaming.api.java.JavaPairDStream;\n  import org.apache.spark.streaming.api.java.JavaReceiverInputDStream;\n  import org.apache.spark.streaming.api.java.JavaStreamingContext;\n  import scala.Tuple2;\n  \n  import java.util.Arrays;\n  import java.util.Iterator;\n  \n  \n  /**\n   * #author      : ivanl001\n   * #creator     : 2018-12-01 14:19\n   * #description : java版本\n   **/\n  public class A1001_SparkStreamDemo_java {\n  \n      public static void main(String[] args) throws Exception {\n  \n  \n          //01, 创建配置对象\n          SparkConf conf = new SparkConf();\n          conf.setAppName(\"java_stream\");\n          //conf.setMaster(\"local[2]\");\n          conf.setMaster(\"spark://master:7077\");\n  \n          //02, 创建流对象上下文,注意设置间隔时间哦\n          JavaStreamingContext streamingContext = new JavaStreamingContext(conf, Seconds.apply(10));\n  \n          //03, 创建nc文件输入\n          JavaReceiverInputDStream inputDStream =  streamingContext.socketTextStream(\"master\", 9999);\n  \n          //04, 切割数据, 这里的JavaReceiverInputDStream可以使用flatMap方法不是特别懂\n          /*JavaDStream<String> wordDS = inputDStream.flatMap(new FlatMapFunction<String, String>() {\n               @Override\n               public Iterator<String> call(String s) throws Exception {\n                   System.out.println(\"ivanl001--\");\n                   List<String> flatList = new ArrayList<String>();\n                   String[] splitedW = s.split(\" \");\n                   for (String word : splitedW) {\n                       flatList.add(word);\n                   }\n                   return flatList.iterator();\n               }\n           });*/\n          //上面的和这里的是相同的\n          JavaDStream<String> words = inputDStream.flatMap(\n              new FlatMapFunction<String, String>() {\n                  @Override public Iterator<String> call(String x) {\n                      return Arrays.asList(x.split(\" \")).iterator();\n                  }\n              });\n  \n          //05,映射后进行聚合并打印\n          JavaPairDStream<String, Integer> pairDStream = words.mapToPair(new PairFunction<String, String, Integer>() {\n              @Override\n              public Tuple2<String, Integer> call(String s) throws Exception {\n                  return new Tuple2<>(s, 1);\n              }\n          });\n  \n          JavaPairDStream<String, Integer> resultPair = pairDStream.reduceByKey(new Function2<Integer, Integer, Integer>() {\n              @Override\n              public Integer call(Integer v1, Integer v2) throws Exception {\n                  return v1+v2;\n              }\n          });\n  \n          //06,打印\n          resultPair.print();\n          streamingContext.start();              // Start the computation\n          streamingContext.awaitTermination();   // Wait for the computation to terminate\n      }\n  }\n  ```\n* 03, 提交到服务器上运行\n  > spark-submit --master spark://master:7077 --name stream01  --class im.ivanl001.bigData.Spark.A10_SparkStream.A1001_SparkStreamDemo Spark_Test.jar\n\n  > spark-submit --master spark://master:7077 --name stream01  --class im.ivanl001.bigData.Spark.A10_SparkStream.A1001_SparkStreamDemo_java Spark_Test.jar"
    }
  ]
}