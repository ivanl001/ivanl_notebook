{
  "title": "13-Spark-0103-spark简单使用",
  "cells": [
    {
      "type": "markdown",
      "data": "## 1，Spark安装\n* 01，需要注意一点就是spark安装的时候需要对应版本，如果版本不一致很可能造成一些安装问题\n* 02，其他步骤都是类似，下载安装包，解压，然后配置环境变量等等，不再赘述\n\n## 2，Spark的介绍\n\n* 01，进入Spark命令行，这里是进入本地模式哈\n  > spark-shell\n  > 本地模式ui地址：http://master:4040\n  > 顺便说一下，分布式的是:http://master:8080\n\n* 02, sc对象\n  > sc是sparkContext，也就是spark上下文对象, spark程序的入口点，封装了整个spark运行环境信息\n\n* 03，RDD\n  > 弹性分布式数据集\n\n## 3, Spark使用案例(命令行内)\n\n* 01，wordcount案例， 一句搞定式：\n  ```scala\n  //第一步读取文件\n  val rdd01 = sc.textFile(\"root/test/zhang.txt\")\n  //第二步按照空格压平，可以直接简化成第二行\n  //val rdd02 = rdd01.flatMap(line => line.split(\" \"))\n  val rdd02 = rdd01.flatMap(_.split(\" \"))\n  //第三步映射\n  val rdd03 = rdd02.map(word => (word, 1))\n  //第四步按key聚合\n  val rdd04 = rdd03.reduceByKey(_ + _)\n  //最后查看结果\n  rdd04.collect\n  //res2: Array[(String, Int)] = Array((are,1), (is,2), (man!,1), (You,1), (\"\",2), (of,2), (ivanl002,1), (world!,2), (ivanl001,1), (awesome,,1), (king,2), (the,2))\n  \n  \n  //可以写成如下一行：\n  val resultRdd = sc.textFile(\"/root/test/zhang.txt\").flatMap(line => line.split(\" \")).map(word => (word, 1)).reduceByKey(_ + _)\n  resultRdd.collect\n  ```\n\n* 02, wordcount加上过滤操作\n  ```scala\n  val resultRdd = sc.textFile(\"/root/test/zhang.txt\").flatMap(line => line.split(\" \")).map(word => (word, 1)).reduceByKey(_ + _)\n  resultRdd.collect\n  ```"
    },
    {
      "type": "markdown",
      "data": "## 4, scala代码编程实现wordcount\n*SparkContext:Spark功能的主要入口点。代表到Spark集群的连接，可以创建RDD、累加器和广播变量.每个JVM只能激活一个SparkContext对象，在创建sc之前需要stop掉active的sc。*\n  ```scala\n  package im.ivanl001.bigData.Spark.A01_WordCount\n\n  import org.apache.spark.{SparkConf, SparkContext}\n  \n  //wordcount主程序\n  object WordCountApp_scala {\n  \n    def main(args: Array[String]): Unit = {\n  \n      //创建Spark配置对象\n      val conf = new SparkConf()\n      conf.setAppName(\"WordCountScala\")\n      //设置master属性\n      conf.setMaster(\"local\") ;\n  \n      //通过conf创建sc\n      val sc = new SparkContext(conf)\n  \n      //加载文本文件\n      val rdd1 = sc.textFile(args(0))\n      //压扁,可以直接简化成第二行\n      //val rdd2 = rdd1.flatMap(line => line.split(\" \")) ;\n      val rdd02 = rdd01.flatMap(_.split(\" \"))\n\n      //映射w => (w,1)\n      val rdd3 = rdd2.map((_,1))\n      val rdd4 = rdd3.reduceByKey(_ + _)\n      val r = rdd4.collect()\n      //r.foreach(println)\n      r.foreach(println(_))\n    }\n  }\n  ```"
    },
    {
      "type": "markdown",
      "data": "## 5, java代码实现wordcount\n\n```java\npackage im.ivanl001.bigData.Spark.A01_WordCount;\n\nimport org.apache.spark.SparkConf;\nimport org.apache.spark.api.java.JavaPairRDD;\nimport org.apache.spark.api.java.JavaRDD;\nimport org.apache.spark.api.java.JavaSparkContext;\nimport org.apache.spark.api.java.function.FlatMapFunction;\nimport org.apache.spark.api.java.function.Function2;\nimport org.apache.spark.api.java.function.PairFunction;\nimport scala.Int;\nimport scala.Tuple2;\n\nimport java.util.ArrayList;\nimport java.util.Iterator;\nimport java.util.List;\n\n/**\n * #author      : ivanl001\n * #creator     : 2018-11-19 19:54\n * #description :\n **/\npublic class WordCountApp_java {\n\n\n    public static void main(final String[] args) {\n\n        //1，首先设置配置信息\n        SparkConf sparkConf = new SparkConf();\n        sparkConf.setAppName(\"WordCountApp_java\");\n        sparkConf.setMaster(\"local\");\n\n        //2, 利用配置信息创建spark上下文对象\n        JavaSparkContext context = new JavaSparkContext(sparkConf);\n\n        //3，读取文件，获取数据集\n        JavaRDD<String> rdd01 = context.textFile(\"/Users/ivanl001/Desktop/bigData/input/zhang.txt\");\n\n        //4, 对获取到到数据集压平处理\n        JavaRDD<String> rdd02 = rdd01.flatMap(new FlatMapFunction<String, String>() {\n            public Iterator<String> call(String s) throws Exception {\n                List<String> flatList = new ArrayList<String>();\n                String[] arr = s.split(\" \");\n                for (String str : arr) {\n                    flatList.add(str);\n                }\n                return flatList.iterator();\n            }\n        });\n\n        //5，对压平后对数据进行映射处理，方便后续进行计算数量\n        JavaPairRDD<String, Integer> rdd03 = rdd02.mapToPair(new PairFunction<String, String, Integer>() {\n            public Tuple2<String, Integer> call(String s) throws Exception {\n                return new Tuple2<String, Integer>(s, 1);\n            }\n        });\n\n        //6，对映射后对数据集进行byKey聚合操作\n        JavaPairRDD<String, Integer> rdd04 = rdd03.reduceByKey(new Function2<Integer, Integer, Integer>() {\n            public Integer call(Integer v1, Integer v2) throws Exception {\n                return v1+v2;\n            }\n        });\n\n        //7, 处理结果，打印出来即可\n        List<Tuple2<String, Integer>> result = rdd04.collect();\n        for (Tuple2<String, Integer> tuple2 : result) {\n            System.out.println(\"key:\" + tuple2._1 + \"----and value:\"  + tuple2._2);\n        }\n    }\n}\n```"
    },
    {
      "type": "markdown",
      "data": "> 注意问题,如果报错如下，一般是因为scala编译器版本的问题：\n```java\njava.lang.NoSuchMethodError: scala.Predef$.refArrayOps([Ljava/lang/Object;)Lscala/collection/mutable/ArrayOps;\n```\n> 解决办法：\n```java\n将scala-sdk从2.12换为2.11\nFile -> Project Structure -> Global libraries -> Remove SDK -> Rebuild.\n```"
    }
  ]
}