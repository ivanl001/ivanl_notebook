{
  "title": "13-Spark-0510-SparkStream模块下集成kafka",
  "cells": [
    {
      "type": "markdown",
      "data": "* 01，首先添加maven依赖\n  ```xml\n  <!--spark-stream计算需要的架包-->\n  <dependency>\n      <groupId>org.apache.spark</groupId>\n      <artifactId>spark-streaming_2.11</artifactId>\n      <version>2.1.0</version>\n  </dependency>\n\n  <dependency>\n      <groupId>org.apache.spark</groupId>\n      <artifactId>spark-streaming-kafka-0-10_2.11</artifactId>\n      <version>2.1.0</version>\n  </dependency>\n  ```\n  \n* 02, 代码如下: 运行之前先去kafka创建主题并开启kafka的生产者\n\n  > kafka-console-producer.sh --broker-list slave01:9092 --topic test\n\n  ```java\n  package im.ivanl001.bigData.Spark.A11_SparkStream_kafka;\n  \n  import org.apache.kafka.clients.consumer.ConsumerRecord;\n  import org.apache.kafka.common.serialization.StringDeserializer;\n  import org.apache.spark.SparkConf;\n  import org.apache.spark.api.java.function.FlatMapFunction;\n  import org.apache.spark.api.java.function.Function2;\n  import org.apache.spark.api.java.function.PairFunction;\n  import org.apache.spark.streaming.Seconds;\n  import org.apache.spark.streaming.api.java.JavaDStream;\n  import org.apache.spark.streaming.api.java.JavaInputDStream;\n  import org.apache.spark.streaming.api.java.JavaPairDStream;\n  import org.apache.spark.streaming.api.java.JavaStreamingContext;\n  import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n  import org.apache.spark.streaming.kafka010.KafkaUtils;\n  import org.apache.spark.streaming.kafka010.LocationStrategies;\n  import scala.Tuple2;\n  \n  import java.util.*;\n  \n  /**\n   * #author      : ivanl001\n   * #creator     : 2018-12-01 17:52\n   * #description : java版的集成kafka\n   **/\n  public class A1101_Stream_kafka_java {\n  \n      public static void main(String[] args) throws Exception {\n  \n  \n          //01, 创建配置对象\n          SparkConf conf = new SparkConf();\n          conf.setAppName(\"java_stream\");\n          conf.setMaster(\"local[2]\");\n          //conf.setMaster(\"spark://master:7077\");\n  \n          //02, 创建流对象上下文,注意设置间隔时间哦\n          JavaStreamingContext streamingContext = new JavaStreamingContext(conf, Seconds.apply(10));\n  \n          //03, 配置kafka的参数\n          Map<String, Object> kafkaParams = new HashMap<>();\n          kafkaParams.put(\"bootstrap.servers\", \"slave01:9092,slave02:9092,slave03:9092\");\n          kafkaParams.put(\"key.deserializer\", StringDeserializer.class);\n          kafkaParams.put(\"value.deserializer\", StringDeserializer.class);\n          kafkaParams.put(\"group.id\", \"use_a_separate_group_id_for_each_stream\");\n          kafkaParams.put(\"auto.offset.reset\", \"latest\");\n          kafkaParams.put(\"enable.auto.commit\", false);\n          //这里可以接受多个主题，不过我们这里就暂时还接受一个就足够了\n          //Collection<String> topics = Arrays.asList(\"topicA\", \"topicB\");\n          Collection<String> topics = Arrays.asList(\"test\");\n  \n  \n          JavaInputDStream<ConsumerRecord<String, String>> inputDStream =\n                  KafkaUtils.createDirectStream(\n                          streamingContext,\n                          LocationStrategies.PreferConsistent(),\n                          ConsumerStrategies.<String, String>Subscribe(topics, kafkaParams)\n                  );\n  \n  \n          //这种方式不是很好处理\n          /*JavaPairDStream<String, String> pairDStream = stream.mapToPair(\n                  new PairFunction<ConsumerRecord<String, String>, String, String>() {\n  \n                      @Override\n                      public Tuple2<String, String> call(ConsumerRecord<String, String> record) {\n  \n                          //这里其实只有一个value，是没有key的哈, 这里的value是输入的一行文字\n                          System.out.println(\"the key is :\" + record.key() + \"-------, the value is : \" + record.value());\n  \n                          return new Tuple2<>(record.key(), record.value());\n                      }\n                  });*/\n  \n          //所以我们用第二种方式\n          JavaDStream<String> words = inputDStream.flatMap(new FlatMapFunction<ConsumerRecord<String, String>, String>() {\n              @Override\n              public Iterator<String> call(ConsumerRecord<String, String> stringStringConsumerRecord) throws Exception {\n                  String value = stringStringConsumerRecord.value();\n                  return Arrays.asList(value.split(\" \")).iterator();\n              }\n          });\n  \n          //05,映射后进行聚合并打印\n          JavaPairDStream<String, Integer> pairDStream = words.mapToPair(new PairFunction<String, String, Integer>() {\n              @Override\n              public Tuple2<String, Integer> call(String s) throws Exception {\n                  return new Tuple2<>(s, 1);\n              }\n          });\n  \n          JavaPairDStream<String, Integer> resultPair = pairDStream.reduceByKey(new Function2<Integer, Integer, Integer>() {\n              @Override\n              public Integer call(Integer v1, Integer v2) throws Exception {\n                  return v1+v2;\n              }\n          });\n  \n          resultPair.print();\n  \n          streamingContext.start();\n          streamingContext.awaitTermination();\n      }\n  }\n  ```\n  \n03, 在kafka的生产者中进行输入数据即可在控制台看到处理结果\n  "
    }
  ]
}