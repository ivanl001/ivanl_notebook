{
  "title": "13-Spark2.1-0810-python的站点迭代爬取程序",
  "cells": [
    {
      "type": "markdown",
      "data": "```python\n\n#\n# author      : ivanl001\n# creator     : 2018-12-08 13:59\n# description : 爬虫基础, 这里简单写下代码， 在spark的0810小节有更多的内容，这里基本实现了，但是还是有点小问题，如果网页没有回应，就会一直等待，这个明显是不行的，之后再用到会再解决\n# \n\nimport urllib3\nimport re\nimport time\nimport os\n\n\n# 1，这里是创建连接池\nhttp = urllib3.PoolManager()\n\n\n# 获取一个页面到url\ndef get_url_from_page(page_str):\n    # 1, 解析\n    pattern = u\"<a\\\\s*href=\\\"([\\u0000-\\uffff]*?)\\\".*?>\"\n    # pattern = u'<a[\\u0000-\\uffff&&^[href]]*href=\"([\\u0000-\\uffff&&^\"]*?)\"'\n    urls = re.finditer(pattern, page_str)\n    for url in urls:\n        addr = url.group(1)\n        if str(addr).startswith(\"http\"): # 这里做的比较简单，只判断http开头的，其他的一概不要了\n            print(addr)\n            # 这里要先判断一下这个文件是否已经存在，如果已经存在就不能再下载了\n            name = addr.replace(\":\", \"\").replace(\"//\",\"\").replace(\"/\",\"\")\n            outputPath = \"/Users/ivanl001/Desktop/zhang/\" + name + \".html\"\n            if os.path.exists(outputPath):\n                continue\n            else:\n                # 这里就是相当于循环，一直在不停的循环迭代进行获取网页\n                download_page(addr)\n        else:\n            continue\n\n\n# 保存到文件\ndef download_page(urlStr):\n\n    # 1, 进行请求, 这里应该设置一个超时时间什么的， 因为现在有一个问题，如果其中一个网页无响应，就会卡在那里一直等，这个后续\n    result = http.request(\"GET\", urlStr)\n\n    # 2，获取(打印)结果\n    resultByte = result.data\n    # print(result.data)\n\n    # 3, 对获取结果进行解码，并进行保存到文件，这里命名按照时间先来吧\n    # 这里命名还是不能用时间，要不没法判断是否已经存在\n    # current_time = int(time.time()*100000)\n    name = urlStr.replace(\":\", \"\").replace(\"//\",\"\").replace(\"/\",\"\")\n    outputPath = \"/Users/ivanl001/Desktop/zhang/\" + name + \".html\"\n    print(outputPath)\n    output = open(outputPath ,\"wb\")\n    output.write(resultByte)\n\n    # 5, 对获取页面进行解码，并获取其中对url\n    pageStr = resultByte.decode(\"utf-8\")\n    # print(pageStr)\n    get_url_from_page(pageStr)\n\n\n\n\n\nurl = \"http://focus.tianya.cn/\"\n# download_page(\"http://focus.tianya.cn/\")\ndownload_page(url)\n\n\n```"
    }
  ]
}