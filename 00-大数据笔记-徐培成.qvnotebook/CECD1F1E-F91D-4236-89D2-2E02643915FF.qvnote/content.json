{
  "title": "03-Hadoop-0609-Partitioner分区",
  "cells": [
    {
      "type": "markdown",
      "data": "## 1, 默认分区方法\n![IMAGE](quiver-image-url/86E149F5453892F6922D5665D74216C1.jpg =526x270)\n\n* 上面这个图是分区算法，基本可以理解为获取hash值，然后对reduce的个数进行取余，然后就知道哪些key进入哪些分区了。和整数的最大值做&运算是因为hash值有可能为负数\n\n* 重写分区函数并设置给job之后就可以按照我们重写的分区进行分区\n\n## 2, 分区具体代码如下:\n### 2.1, 重写的分区类\n```java\npackage im.ivanl001.bigData.Hadoop.A06_partion;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Partitioner;\n\n/**\n * #author      : ivanl001\n * #creator     : 2018-10-24 15:09\n * #description : 自定义分区函数\n **/\npublic class IMPartitioner extends Partitioner<Text, IntWritable>{\n\n    //这里的意义就是所有的都会分到0这个分区上，也即是说就算设置了三个reducer也是没用的，所有的都会到0上\n    public int getPartition(Text text, IntWritable intWritable, int i) {\n        return 0;\n    }\n\n}\n\n```\n\n### 2.2，设置给job\n\n```java\npackage im.ivanl001.bigData.Hadoop.A06_partion;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\n\n/**\n * #author      : ivanl001\n * #creator     : 2018-10-20 19:35\n * #description : wordcount\n **/\npublic class IMWordCountApp {\n\n    /*\n     * mapper过程之后产生的文件的命名中是***-m-00000*什么的，m代表是mapper，后面的数字代表是分区\n     * reducer过程之后产生的文件的命名中是***-r-00000*什么的，r代表的是reducer，后面的数字代表的也是分区\n     * 如果设置三个reducer，在没有重写分区函数的情况下，会有三个r，也就会有三个输出文件，因为一个reducer会有一个输出文件\n     * 如果重写了分区函数，其实也会生成三个文件，但是只有算法中有指向的才会有内容，其他的就是空文件了\n     * */\n    public static void main(String[] args) {\n\n        try {\n\n            if (args.length != 2) {\n                System.out.println(\"参数个数有误！\");\n                return;\n            }\n\n            //\"/users/ivanl001/Desktop/bigData/input/zhang.txt\"\n            String inputFileStr = args[0];\n            String outputFolderStr = args[1];\n\n            //0，创建配置对象，以修正某些配置文件中的配置\n            Configuration configuration = new Configuration();\n            //这里一旦设置单机版就会出错，而且不能有core-default.xml文件，这个文件中一旦配置也会有问题，不知道为啥，先过\n//            configuration.set(\"fs.defaultFS\", \"file:///\");\n            //configuration.set(\"fs.hdfs.impl\", org.apache.hadoop.hdfs.DistributedFileSystem.class.getName());\n            //configuration.set(\"fs.file.impl\", org.apache.hadoop.fs.LocalFileSystem.class.getName());\n\n            //这里因为文件存在，总是需要删除，麻烦，所以直接程序自动删除\n            FileSystem.get(configuration).delete(new Path(outputFolderStr));\n\n            //1，创建作业\n            Job wordcountJob = Job.getInstance(configuration);\n            wordcountJob.setJobName(\"wordcountApp\");\n            //之前这句没写，就会一直报错，什么mapper类找不到，这里需要注意一下\n            wordcountJob.setJarByClass(IMWordCountApp.class);\n\n            //--------------------------------这里设置输出格式类------------------------------------\n            //这个是设置输出格式为------------序列文件输出格式-------------，我这里并不想保存序列文件，所以这里就不设置\n            //wordcountJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n\n\n            //2,设置作业输入\n            //这句话可以不加，因为默认就是文本输入格式\n            wordcountJob.setInputFormatClass(TextInputFormat.class);\n            FileInputFormat.addInputPath(wordcountJob, new Path(inputFileStr));\n\n            //3，设置mapper\n            wordcountJob.setMapperClass(IMWordCountMapper.class);\n            wordcountJob.setMapOutputKeyClass(Text.class);\n            wordcountJob.setMapOutputValueClass(IntWritable.class);\n\n            //----中间设置一下分区函数\n            wordcountJob.setPartitionerClass(IMPartitioner.class);\n\n\n            //4, 设置reducer\n            wordcountJob.setReducerClass(IMWordCountReducer.class);\n            //每个reduce会产生一个输出结果或者输出文件，这里设置一个reduce\n            wordcountJob.setNumReduceTasks(3);//设置reducer的个数，如果是0就是不需要r\n            //设置输出的key和value的类型\n            wordcountJob.setOutputKeyClass(Text.class);\n            wordcountJob.setOutputValueClass(IntWritable.class);\n\n            //5, 设置输出\n            wordcountJob.setOutputValueClass(FileOutputFormat.class);\n            FileOutputFormat.setOutputPath(wordcountJob, new Path(outputFolderStr));\n\n            //6，提交，开始处理\n            wordcountJob.waitForCompletion(false);\n\n        } catch (IOException e) {\n            e.printStackTrace();\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        } catch (ClassNotFoundException e) {\n            e.printStackTrace();\n        }\n    }\n}\n```\n"
    }
  ]
}