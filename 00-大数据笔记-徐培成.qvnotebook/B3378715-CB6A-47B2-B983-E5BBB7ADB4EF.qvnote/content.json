{
  "title": "03-Hadoop-0705-二次排序",
  "cells": [
    {
      "type": "markdown",
      "data": "* `参考  290/756  [O'REILLY]Hadoop.The.Definitive.Guide.4th.Edition.2015.3`\n\n*简单说一下：二次排序其实就是对value进行排序，但是value又不能排序。所以我们可以自定义一个模型，把value放在模型中，然后把模型作在map中映射成key，这样子就可以对value进行排序了*\n\n*下面的二次排序已经可以很大程度上解决reducer端的数据计算问题，因为这里到reducer已经排好序，直接取出来就好了。先这么说明，好像也有点不对*\n\n*?突然想到一个问题：二次排序如何避免数据倾斜呢？*\n"
    },
    {
      "type": "markdown",
      "data": "## 二次排序\n\n### 1,先定义模型，也就是组合key，value放在其中\n\n```java\npackage im.ivanl001.bigData.Hadoop.A12_maxTemp_2nd_sort;\n\nimport org.apache.hadoop.io.WritableComparable;\n\nimport java.io.DataInput;\nimport java.io.DataOutput;\nimport java.io.IOException;\n\n/**\n * #author      : ivanl001\n * #creator     : 2018-10-25 10:45\n * #description : 自定义key\n *              : 二次排序的核心就在这里，通过自定义的key，把value隐藏在其中，就能实现对value的排序\n **/\npublic class CombinedKey implements WritableComparable<CombinedKey> {\n\n    private int key;\n\n    private int value;\n\n    //这里比较的意思就是年份升序，气温降序\n    public int compareTo(CombinedKey o) {\n\n        int otherKey = o.getKey();\n        int otherValue = o.getValue();\n\n        if (key != otherKey) {\n            return (key - otherKey);\n        }else {\n            return -(value - otherValue);\n        }\n    }\n\n    /*public int compareTo(Object o) {\n        return this.compareTo((CombinedKey) o);\n    }*/\n\n    public void write(DataOutput out) throws IOException {\n        out.writeInt(key);\n        out.writeInt(value);\n    }\n\n    public void readFields(DataInput in) throws IOException {\n        key = in.readInt();\n        value = in.readInt();\n    }\n\n    public int getKey() {\n        return key;\n    }\n\n    public void setKey(int key) {\n        this.key = key;\n    }\n\n    public int getValue() {\n        return value;\n    }\n\n    public void setValue(int value) {\n        this.value = value;\n    }\n}\n\n```"
    },
    {
      "type": "markdown",
      "data": "### 2, 我们的mapper如下\n\n```java\npackage im.ivanl001.bigData.Hadoop.A12_maxTemp_2nd_sort;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.mapreduce.Mapper;\n\nimport java.io.IOException;\n\n/**\n * #author      : ivanl001\n * #creator     : 2018-10-24 18:50\n * #description :\n **/\npublic class IMMaxTempMapper extends Mapper<IntWritable, IntWritable, CombinedKey, NullWritable> {\n\n    @Override\n    protected void map(IntWritable key, IntWritable value, Context context) throws IOException, InterruptedException {\n        System.out.println(\"key:\" + key + \",value:\" + value);\n\n        CombinedKey combinedKey = new CombinedKey();\n        combinedKey.setKey(key.get());\n        combinedKey.setValue(value.get());\n        context.write(combinedKey, NullWritable.get());\n    }\n\n}\n```"
    },
    {
      "type": "markdown",
      "data": "### 3, 然后是分区类\n\n```java\npackage im.ivanl001.bigData.Hadoop.A12_maxTemp_2nd_sort;\n\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.mapreduce.Partitioner;\n\n/**\n * #author      : ivanl001\n * #creator     : 2018-10-24 21:31\n * #description : 根据年份进行分区,因为是对mapper的输出进行分区，所以这里的输入就应该是mapper的输出\n **/\npublic class IMYearPatitioner extends Partitioner<CombinedKey, NullWritable> {\n\n    public int getPartition(CombinedKey key, NullWritable value, int i) {\n\n        int year = key.getKey();\n\n        if (year < 2000) {\n            return 0;\n        } else if (year < 2030) {\n            return 1;\n        } else {\n            return 2;\n        }\n    }\n}\n```"
    },
    {
      "type": "markdown",
      "data": "### 4, 模型的比较方法需要自己写一下，默认情况下key不是排序的，所以我们需要先进行排序。然后才是分组\n\n```java\npackage im.ivanl001.bigData.Hadoop.A12_maxTemp_2nd_sort;\n\nimport org.apache.hadoop.io.WritableComparable;\nimport org.apache.hadoop.io.WritableComparator;\n\n/**\n * #author      : ivanl001\n * #creator     : 2018-10-25 12:24\n * #description : 用来比较两个CombinedKey的顺序\n *              : 这个类的作用是为了设置到job中去， 因为对比器你又不能直接用模型中的方法，所以需要重新定义一个类放进去\n **/\npublic class IMCombinedKeyComparator extends WritableComparator {\n\n    protected IMCombinedKeyComparator() {\n        super(CombinedKey.class, true);\n    }\n\n    @Override\n    public int compare(WritableComparable w1, WritableComparable w2) {\n    \n        CombinedKey key01 = (CombinedKey) w1;\n        CombinedKey key02 = (CombinedKey) w2;\n\n        return key01.compareTo(key02);\n    }\n}\n```"
    },
    {
      "type": "markdown",
      "data": "### 5, 重新分组方法，同一年的需要放在同一组中\n\n```java\npackage im.ivanl001.bigData.Hadoop.A12_maxTemp_2nd_sort;\n\nimport org.apache.hadoop.io.WritableComparable;\nimport org.apache.hadoop.io.WritableComparator;\n\n/**\n * #author      : ivanl001\n * #creator     : 2018-10-25 12:16\n * #description : 分组\n **/\npublic class IMYearGroupSeparator extends WritableComparator {\n\n    protected IMYearGroupSeparator() {\n        super(CombinedKey.class, true);\n    }\n\n    @Override\n    public int compare(WritableComparable w1, WritableComparable w2) {\n\n        /*IntPair ip1 = (IntPair) w1;\n        IntPair ip2 = (IntPair) w2;\n        return IntPair.compare(ip1.getFirst(), ip2.getFirst());*/\n\n        CombinedKey key01 = (CombinedKey) w1;\n        CombinedKey key02 = (CombinedKey) w2;\n\n        //因为分组的话，只能根据年份分组，也就是CombinedKey的key值进行分组\n        return (key01.getKey() - key02.getKey());\n    }\n\n}\n```"
    },
    {
      "type": "markdown",
      "data": "### 6, reducer\n\n```java\npackage im.ivanl001.bigData.Hadoop.A12_maxTemp_2nd_sort;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.mapreduce.Reducer;\n\nimport java.io.IOException;\n\n/**\n * #author      : ivanl001\n * #creator     : 2018-10-24 18:51\n * #description :\n **/\npublic class IMMaxTempReducer extends Reducer<CombinedKey, NullWritable, IntWritable, IntWritable> {\n\n    @Override\n    protected void reduce(CombinedKey key, Iterable<NullWritable> values, Context context) throws IOException, InterruptedException {\n\n        int year = key.getKey();\n        int temp = key.getValue();\n\n        System.out.println(\"-----------------reducer-----------------\");\n\n        for (NullWritable nullWritable : values) {\n            System.out.println(\"year: \" + key.getKey() + \", temp: \" + key.getValue());\n        }\n        context.write(new IntWritable(year), new IntWritable(temp));\n    }\n}\n```"
    },
    {
      "type": "markdown",
      "data": "### 7, 最后设置app就好\n\n```java\npackage im.ivanl001.bigData.Hadoop.A12_maxTemp_2nd_sort;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\n/**\n * #author      : ivanl001\n * #creator     : 2018-10-24 18:53\n * #description :\n **/\n\n//-------------------二次排序是不是不能使用采样？如果使用采样，怎么能定义一下采样的\n\npublic class IMMaxTempApp {\n\n    public static void main(String[] args) throws Exception {\n\n        //1，先判断参数，如果通过则取出参数\n        if (args.length != 2) {\n            System.out.println(\"参数个数有误！\");\n            return;\n        }\n        String inputFileStr = args[0];\n        String outputFolderStr = args[1];\n\n        //2，获取文件系统，并删除输出文件夹以免出错\n        Configuration configuration = new Configuration();\n        FileSystem fileSystem = FileSystem.get(configuration);\n        fileSystem.delete(new Path(outputFolderStr));\n\n        //3,创建作业, 并设置基本的选项\n        Job maxTempJob = Job.getInstance(configuration);\n        maxTempJob.setJobName(\"maxTempJob\");\n        maxTempJob.setJarByClass(IMMaxTempApp.class);\n        //因为采用的是序列文件，所以这里需要设置一下\n        maxTempJob.setInputFormatClass(SequenceFileInputFormat.class);\n\n        //4, 输入\n        FileInputFormat.addInputPath(maxTempJob, new Path(inputFileStr));\n\n\n        //5, 设置mapper\n        maxTempJob.setMapperClass(IMMaxTempMapper.class);\n        //这里其实是有点蠢的，其实在IMMaxTempMapper中已经指定了输出的key和value的类型了，但是这里还是需要指定一下，要不然就有可能不对\n        maxTempJob.setMapOutputKeyClass(CombinedKey.class);\n        maxTempJob.setMapOutputValueClass(NullWritable.class);\n\n        //6, 设置reducer\n        //注意：如果有多个r的话(一般都会有多个r)，那么这样的就不属于全排序了，因为不同的年份会分布在不同的r中处理，也就会有多个文件输出\n        maxTempJob.setReducerClass(IMMaxTempReducer.class);\n        maxTempJob.setNumReduceTasks(3);\n\n        //设置分区函数，自定义分区是ok的，那我们再试试用全排序加采样进行试试\n        maxTempJob.setPartitionerClass(IMYearPatitioner.class);\n\n\n        //排序和分组都是在mapper之后进行的，先进行排序，排序之后在进行分组\n\n        //设置排序\n        maxTempJob.setSortComparatorClass(IMCombinedKeyComparator.class);\n\n        //设置分组\n        maxTempJob.setGroupingComparatorClass(IMYearGroupSeparator.class);\n\n        //------------------------注意：采样要放在分区之后，否则会报错什么找不到分区文件，这种问题其实感觉应该是bug，不然为啥要放在后面，本来采样就是在r之前的啊------------------------------------\n        /*\n\n        //7.1, 设置分区函数,这里还是不能用自己定义的分区函数，因为你用自己定义的分区函数，采样的意义也就没有了\n        //maxTempJob.setPartitionerClass(IMYearPatitioner.class);\n        maxTempJob.setPartitionerClass(TotalOrderPartitioner.class);\n        //如果需要指定位置的话，需要注意一下：设置的时候使用configuration是不能直接使用的，而是需要从maxTempJob.getConfiguration()取出来使用，因为job里面是重新拷贝了一份\n        //TotalOrderPartitioner.setPartitionFile(maxTempJob.getConfiguration(), new Path(\"/Users/ivanl001/Desktop/bigData/output005/partitioner.lst\"));\n\n        //7.2, 设置采样\n        //说明一下，因为采样是对key进行采样的，所以只好用seqfile，要不然比如说用的是文本文件格式， key是偏移量，采样就没有什么意义了\n        //这句话暂时不知道为什么要设置，先注释\n        //maxTempJob.setPartitionerClass(TotalOrderPartitioner.class);\n        InputSampler.Sampler<IntWritable, IntWritable> sampler = new InputSampler.RandomSampler<IntWritable, IntWritable>(0.1, 10000, 10);\n        InputSampler.writePartitionFile(maxTempJob, sampler);\n\n        //----------呀呀呀呀，采样真的很好用哈，哈哈哈哈---------------\n\n        *//*Configuration conf = maxTempJob.getConfiguration();\n        String partitionFile = TotalOrderPartitioner.getPartitionFile(conf);\n        URI partitionUri = new URI(partitionFile);\n        maxTempJob.addCacheFile(partitionUri);*//*\n        */\n\n        //7，输出\n        FileOutputFormat.setOutputPath(maxTempJob, new Path(outputFolderStr));\n\n        //8, 提交，开始处理\n        maxTempJob.waitForCompletion(false);\n    }\n}\n```"
    }
  ]
}