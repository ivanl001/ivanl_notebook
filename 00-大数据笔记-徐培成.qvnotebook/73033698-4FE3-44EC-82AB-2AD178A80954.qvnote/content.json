{
  "title": "13-Spark-0210-RDD动作",
  "cells": [
    {
      "type": "markdown",
      "data": "## 2, RDD Action\n\n\n### 01,collect()\n\n*其实有点不太懂collect的作用了，因为实验了一下，就算没有collect，foreach依然能够取出rdd的值，那要collect做什么嘞-----哈哈哈，明白了， 因为虽然没有用collect，但是用了foreach，foreach也是一个动作！！！*\n\n*Returns all the elements of the dataset as an array at the driver program. This is usually useful after a filter or other operation that returns a sufficiently small subset of the data.*\n\n\n```scala\nprintln(\"_________________________________________________\")\n//val r = rdd4.collect()\n//r.foreach(println)\n//r.foreach(println(_))\n\nrdd4.foreach(e => {\nprintln(\"no collect++++++++++++\")\nprintln(e)\n})\n```\n\n\n### 02, reduce(func)\n\n*注意：reduceByKey虽然是变换，但是reduce是动作哈*\n\n```scala\nval rdd4 = rdd3.reduceByKey(_ + _)\n//在这里可以再对结果的rdd04再map一次获取所有单词的个数，如下\nval rdd5 = rdd4.map(t => t._2)//这里把每个单词的第二个个数取出来，后面再聚合一次就可以得到所有的个数了\nval count = rdd5.reduce(_+_)\n\nprintln(\"个数是：\"+count)\n```\n\n\n### 03, count()\n\n*比较简单，就会得出总数*\n\n```scala\n//压扁\nval rdd2 = rdd1.flatMap(line => {\n  println(\"ivanl001\" + line)\n  line.split(\" \")\n})\n\nprintln(\"个数是：\" + rdd2.count())\n```\n\n### 04, 其他的\n```scala\npackage im.ivanl001.bigData.Spark.A03_Action\nimport org.apache.spark.{SparkConf, SparkContext}\n\nobject A0301_Action_first {\n\n  def main(args: Array[String]): Unit = {\n\n    //创建Spark配置对象\n    val conf = new SparkConf()\n\n    //集群模式下下面两行不要\n    conf.setAppName(\"WordCountScala\")\n    //设置master属性\n    //conf.setMaster(\"spark://master:7077\")\n    conf.setMaster(\"local[2]\")//数字是本地模式下开启几个线程模拟多线程\n\n    //通过conf创建sc\n    val sc = new SparkContext(conf)\n\n    //加载文本文件\n    //val rdd1 = sc.textFile(args(0))\n    val rdd1 = sc.textFile(\"/Users/ivanl001/Desktop/bigData/input/zhang.txt\", 4)//数字代表分区\n\n\n    //动作first是取出第一个元素，rdd1中现在是每行文字，所以first也即是取出第一行文字\n    println(\"-------------------------first-------------------------\")\n    val firstLine = rdd1.first()\n    println(firstLine)\n\n    //其实first就是调用的take，我们可以直接调用take动作更好\n    println(\"-------------------------take-------------------------\")\n    val firstTwoLines = rdd1.take(2)\n    firstTwoLines.foreach(println(_))\n\n    //保存成文件，和hadoop的一样\n    println(\"-------------------------saveAsTextFile-------------------------\")\n    rdd1.flatMap(_.split(\" \")).saveAsTextFile(\"/Users/ivanl001/Desktop/bigData/input/out/\")\n\n    //保存成序列文件,只有有key-value对的rdd才能保存成序列文件哈\n    println(\"-------------------------saveAsSequenceFile-------------------------\")\n    rdd1.flatMap(_.split(\" \")).map((_,1)).saveAsSequenceFile(\"/Users/ivanl001/Desktop/bigData/input/out01/\")\n\n    //countByKey()，也就是计算key出现的次数，注意：这个跟reduceByKey效果上的差别是：如果是 （key,1)这样的，结果一直。但是如果是(key,2)这样的值不是1的，countByKey()可不是总的次数，而是key值出现的次数哈\n    println(\"-------------------------countByKey-------------------------\")\n    val rdd2 = rdd1.flatMap(_.split(\" \")).map((_,3))\n    rdd2.countByKey().foreach(println(_))\n  }\n}\n```\n\n\n\n"
    }
  ]
}