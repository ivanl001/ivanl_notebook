{
  "title": "03-Hadoop-0803-MR的链条化",
  "cells": [
    {
      "type": "markdown",
      "data": "*有一点需要注意：在链条化中，可以又很多很多的Mapper，因为Mapper是在一个节点上进行的。但是Reducer一个Job中只能有一个，因为reducer涉及到shuffle，也就是很多节点上的数据，所以不能有多个reducer*"
    },
    {
      "type": "markdown",
      "data": "* 我们这里链条化的案例是这样的：\n* IMWordCountMapper负责map成key value对\n* IMWordCount_M_chain01负责删除某些不符合要求的kv对\n* IMWordCountReducer负责进行加和\n* IMWordCount_R_chain01负责对结果再进行加上某些值。这里这个链条依然是Mapper哈，具体看代码："
    },
    {
      "type": "markdown",
      "data": "## 1，IMWordCountApp\n```java\npackage im.ivanl001.bigData.Hadoop.A14_MR_chain;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.chain.ChainMapper;\nimport org.apache.hadoop.mapreduce.lib.chain.ChainReducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\n\n/**\n * #author      : ivanl001\n * #creator     : 2018-10-20 19:35\n * #description : wordcount\n **/\npublic class IMWordCountApp {\n\n    public static void main(String[] args) {\n\n        try {\n\n            if (args.length != 2) {\n                System.out.println(\"参数个数有误！\");\n                return;\n            }\n\n            //\"/users/ivanl001/Desktop/bigData/input/zhang.txt\"\n            String inputFileStr = args[0];\n            String outputFolderStr = args[1];\n\n            //0，创建配置对象，以修正某些配置文件中的配置\n            Configuration configuration = new Configuration();\n            //这里一旦设置单机版就会出错，而且不能有core-default.xml文件，这个文件中一旦配置也会有问题，不知道为啥，先过\n//            configuration.set(\"fs.defaultFS\", \"file:///\");\n            //configuration.set(\"fs.hdfs.impl\", org.apache.hadoop.hdfs.DistributedFileSystem.class.getName());\n            //configuration.set(\"fs.file.impl\", org.apache.hadoop.fs.LocalFileSystem.class.getName());\n\n            //这里因为文件存在，总是需要删除，麻烦，所以直接程序自动删除\n            FileSystem.get(configuration).delete(new Path(outputFolderStr));\n\n            //1，创建作业\n            Job wordcountJob = Job.getInstance(configuration);\n            wordcountJob.setJobName(\"wordcountApp\");\n            //之前这句没写，就会一直报错，什么mapper类找不到，这里需要注意一下\n            wordcountJob.setJarByClass(IMWordCountApp.class);\n\n            //2,设置作业输入\n            //这句话可以不加，因为默认就是文本输入格式\n            wordcountJob.setInputFormatClass(TextInputFormat.class);\n            FileInputFormat.addInputPath(wordcountJob, new Path(inputFileStr));\n\n            //这个链条主要做了两件事情：------------------下面是链条-----------------------\n            //1，在M的链条中，删除了hellohello字段\n            //2, 在R的链条中，对于超过100的字段进行直接+1000\n            //因为我们这里使用的链条，所以就不需要设置mapper和reducer了\n            ChainMapper.addMapper(wordcountJob, IMWordCountMapper.class, LongWritable.class, Text.class, Text.class, IntWritable.class, configuration);\n            ChainMapper.addMapper(wordcountJob, IMWordCount_M_chain01.class, Text.class, IntWritable.class, Text.class, IntWritable.class, configuration);\n\n            ChainReducer.setReducer(wordcountJob, IMWordCountReducer.class, Text.class, IntWritable.class, Text.class, IntWritable.class, configuration);\n            ChainReducer.addMapper(wordcountJob, IMWordCount_R_chain01.class, Text.class, IntWritable.class, Text.class, IntWritable.class, configuration);\n\n            //不过还是需要设置一下reducer的个数\n            wordcountJob.setNumReduceTasks(1);\n\n\n            /*//3，设置mapper\n            wordcountJob.setMapperClass(IMWordCountMapper.class);\n            wordcountJob.setMapOutputKeyClass(Text.class);\n            wordcountJob.setMapOutputValueClass(IntWritable.class);\n\n\n            //4, 设置reducer\n            wordcountJob.setReducerClass(IMWordCountReducer.class);\n            //每个reduce会产生一个输出结果或者输出文件，这里设置一个reduce\n            wordcountJob.setNumReduceTasks(1);\n            //设置输出的key和value的类型\n            wordcountJob.setOutputKeyClass(Text.class);\n            wordcountJob.setOutputValueClass(IntWritable.class);*/\n\n            //5, 设置输出\n            //wordcountJob.setOutputValueClass(FileOutputFormat.class);\n            FileOutputFormat.setOutputPath(wordcountJob, new Path(outputFolderStr));\n            \n            //6，提交，开始处理\n            wordcountJob.waitForCompletion(false);\n\n        } catch (IOException e) {\n            e.printStackTrace();\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        } catch (ClassNotFoundException e) {\n            e.printStackTrace();\n        }\n    }\n}\n```"
    },
    {
      "type": "markdown",
      "data": "## 2，IMWordCountMapper\n```java\npackage im.ivanl001.bigData.Hadoop.A14_MR_chain;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\nimport java.io.IOException;\n\n/**\n * #author      : ivanl001\n * #creator     : 2018-10-20 19:23\n * #description : mapper\n **/\npublic class IMWordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable>{\n\n    @Override\n    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n        System.out.println(\"key:\" + key + \",value:\" + value);\n        String[] splitStr = value.toString().split(\" \");\n        Text outText = new Text();\n        IntWritable outInt = new IntWritable();\n        for (String str : splitStr) {\n            outText.set(str);\n            outInt.set(1);\n            //这里是意思就是把每个单词拼成(zhang,1), (li, 1), (dan, 1)类似的格式传给reduce\n            context.write(outText, outInt);\n        }\n    }\n}\n```"
    },
    {
      "type": "markdown",
      "data": "## 3,IMWordCount_M_chain01\n```java\npackage im.ivanl001.bigData.Hadoop.A14_MR_chain;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\nimport java.io.IOException;\n\n/**\n * #author      : ivanl001\n * #creator     : 2018-10-25 17:50\n * #description : mapper端的链条1\n **/\npublic class IMWordCount_M_chain01 extends Mapper<Text, IntWritable, Text, IntWritable> {\n\n    @Override\n    protected void map(Text key, IntWritable value, Context context) throws IOException, InterruptedException {\n\n        if (!key.toString().equals(\"hellohello\")) {\n            context.write(key, value);\n        }\n    }\n}\n```"
    },
    {
      "type": "markdown",
      "data": "## 4, IMWordCountReducer\n\n```java\npackage im.ivanl001.bigData.Hadoop.A14_MR_chain;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\nimport java.io.IOException;\n\n/**\n * #author      : ivanl001\n * #creator     : 2018-10-20 19:30\n * #description : reducer\n **/\npublic class IMWordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {\n\n    @Override\n    protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {\n        int count = 0;\n        for (IntWritable intWritable : values) {\n            count = count + intWritable.get();\n        }\n        context.write(key, new IntWritable(count));\n    }\n}\n```"
    },
    {
      "type": "markdown",
      "data": "## 5, IMWordCount_R_chain01\n\n```java\npackage im.ivanl001.bigData.Hadoop.A14_MR_chain;\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\nimport java.io.IOException;\n\n/**\n * #author      : ivanl001\n * #creator     : 2018-10-25 17:52\n * #description : Reducer端的链条1\n **/\npublic class IMWordCount_R_chain01 extends Mapper<Text, IntWritable, Text, IntWritable> {\n\n    @Override\n    protected void map(Text key, IntWritable value, Context context) throws IOException, InterruptedException {\n        if (value.get() > 100) {\n            context.write(key, new IntWritable(value.get() + 1000));\n        }else{\n            context.write(key, value);\n        }\n    }\n}\n```"
    }
  ]
}