{
  "title": "13-Spark-0205-计算单词个数",
  "cells": [
    {
      "type": "markdown",
      "data": "## 计算单词总个数\n\n#### 01，第一个方法是：压扁之后直接求出rdd个数就可以了\n```scala\n//压扁\nval rdd2 = rdd1.flatMap(line => {\n  println(\"ivanl001\" + line)\n  line.split(\" \")\n})\n\nprintln(\"个数是：\" + rdd2.count())\n```\n\n\n#### 02，第二个方法是:在最后reduceByKey后求出每个单词的总数，再加起来啦，嘿嘿嘿\n  \n```scala\nval rdd4 = rdd3.reduceByKey(_ + _)\n//在这里可以再对结果的rdd04再map一次获取所有单词的个数，如下\nval rdd5 = rdd4.map(t => t._2)//这里把每个单词的第二个个数取出来，后面再聚合一次就可以得到所有的个数了\nval count = rdd5.reduce(_+_)\n\nprintln(\"个数是：\"+count)\n```\n\n\n#### 03, 其实还有第三种方式：读取文档后，直接map出每行的长度，进行reduce即可，如果不懂看下面代码*\n\n```scala\nval rdd1 = sc.textFile(\"/Users/ivanl001/Desktop/bigData/input/zhang.txt\", 2)//数字代表分区\n//val rdd1_1 = rdd1.map(line => line.split(\" \").length)\n//上面一行简化成下面代码\nval rdd1_1 = rdd1.map(_.split(\" \").length)\nval count01 = rdd1_1.reduce(_ + _)\nprintln(\"个数是:\" + count01)\n```"
    }
  ]
}