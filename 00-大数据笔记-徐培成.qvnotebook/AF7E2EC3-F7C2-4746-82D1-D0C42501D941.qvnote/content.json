{
  "title": "03-Hadoop-1101-多个文件进行数据连接到两种方式",
  "cells": [
    {
      "type": "markdown",
      "data": "> 1，一张大表，一张小表，Mapper端连接\n\n>2的join先放着，后面有用到再看"
    },
    {
      "type": "markdown",
      "data": "## 1，一张大表，一张小表，Mapper端连接\n*用来右连接的mapper, 如果两个表中有一个比较小，可以放入内存，那么我们可以直接把另外一张表mapper，小表放入内存，然后进行连接，代码如下.因为只是演示，所以这里只写了一个Mapper和主程序app*\n\n### 1.1, Mapper，这个类用来加载小表，并进行连接\n```java\npackage im.ivanl001.bigData.Hadoop.A17_MapperJoin;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.FSDataInputStream;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\nimport java.util.HashMap;\nimport java.util.Map;\n\n//mapreduce是新的接口中的一个包， mapred是老接口的一个包\n\n/**\n * #author      : ivanl001\n * #creator     : 2018-11-01 17:47\n * #description : 用来右连接的mapper, 如果两个表中有一个比较小，可以放入内存，那么我们可以直接把另外一张表mapper，小表放入内存，然后进行连接，代码如下\n **/\npublic class IMJoinMapper extends Mapper<LongWritable, Text, Text, NullWritable> {\n\n    private Map<String, String> users = new HashMap<String, String>();\n\n    //这个方法是map之前会调用的一个方法，其他的准备工作可以在这里完成\n    //我们需要在这里加载小表，并放入到mapper里面\n    @Override\n    protected void setup(Context context) throws IOException, InterruptedException {\n\n        Configuration configuration = context.getConfiguration();\n        FileSystem fileSystem = FileSystem.getLocal(configuration);\n\n        FSDataInputStream fsDataInputStream = fileSystem.open(new Path(\"/Users/ivanl001/Desktop/bigData/join/customers.txt\"));\n\n        //得到字符阅读器\n        BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(fsDataInputStream));\n\n        String line = null;\n        while ((line = bufferedReader.readLine()) != null) {\n            String cid = line.substring(0, line.indexOf(\",\"));\n            String userInfo = line;\n            users.put(cid, userInfo);\n        }\n        //这里在map开始run之前已经把用户信息读入到内存中了，very good\n\n    }\n\n    @Override\n    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n\n        /*String[] splitStr = value.toString().split(\",\");\n        String cid = splitStr[splitStr.length];*/\n\n        String line = value.toString();\n\n        //截取最后一个逗号后面的用户id\n        String cid = line.substring(line.lastIndexOf(\",\") + 1);//默认到最后\n\n        //截取最用一个逗号之前的用户信息\n        String orderInfo = line.substring(0, line.lastIndexOf(\",\"));\n\n        //通过订单中的cid获取内存中的用户信息，拼接到一起\n        String userInfo = users.get(cid);\n        String joinInfo = userInfo + \"----\" + orderInfo;\n\n        context.write(new Text(joinInfo), NullWritable.get());\n    }\n}\n```\n\n### 1.2，App\n\n```java\npackage im.ivanl001.bigData.Hadoop.A17_MapperJoin;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\n\n/**\n * #author      : ivanl001\n * #creator     : 2018-11-01 18:16\n * #description : 这里直接通过mapper进行join\n **/\npublic class IMMapperJoinApp {\n\n    public static void main(String[] args) {\n\n        try {\n            if (args.length != 2) {\n                System.out.println(\"参数个数有误！\");\n                return;\n            }\n\n            //\"/users/ivanl001/Desktop/bigData/input/zhang.txt\"\n            String inputFileStr = args[0];\n            String outputFolderStr = args[1];\n\n            //0，创建配置对象，以修正某些配置文件中的配置\n            Configuration configuration = new Configuration();\n            //这里一旦设置单机版就会出错，而且不能有core-default.xml文件，这个文件中一旦配置也会有问题，不知道为啥，先过\n//            configuration.set(\"fs.defaultFS\", \"file:///\");\n            //configuration.set(\"fs.hdfs.impl\", org.apache.hadoop.hdfs.DistributedFileSystem.class.getName());\n            //configuration.set(\"fs.file.impl\", org.apache.hadoop.fs.LocalFileSystem.class.getName());\n\n            //这里因为文件存在，总是需要删除，麻烦，所以直接程序自动删除\n            FileSystem.get(configuration).delete(new Path(outputFolderStr));\n\n\n            //1，创建作业\n            Job wordcountJob = Job.getInstance(configuration);\n            wordcountJob.setJobName(\"wordcountApp\");\n            //之前这句没写，就会一直报错，什么mapper类找不到，这里需要注意一下\n            wordcountJob.setJarByClass(IMMapperJoinApp.class);\n\n\n            //2,设置作业输入\n            //这句话可以不加，因为默认就是文本输入格式\n            wordcountJob.setInputFormatClass(TextInputFormat.class);\n            FileInputFormat.addInputPath(wordcountJob, new Path(inputFileStr));\n\n            //3，设置mapper\n            wordcountJob.setMapperClass(IMJoinMapper.class);\n            wordcountJob.setMapOutputKeyClass(Text.class);\n            wordcountJob.setMapOutputValueClass(NullWritable.class);\n\n            //4, 设置reducer,这里不需要r，所以就直接全部忽略\n\n\n            //5, 设置输出\n            //wordcountJob.setOutputValueClass(FileOutputFormat.class);\n            FileOutputFormat.setOutputPath(wordcountJob, new Path(outputFolderStr));\n\n            //6，提交，开始处理\n            wordcountJob.waitForCompletion(false);\n\n        } catch (IOException e) {\n            e.printStackTrace();\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        } catch (ClassNotFoundException e) {\n            e.printStackTrace();\n        }\n    }\n}\n```"
    },
    {
      "type": "markdown",
      "data": "## 2，两张大表，reduce端连接\n*reduce端连接其实就是二次排序的主要应用。我们还是拿用户和订单表的连接说明。假设用户表也很大，订单表也很大。那么就需要进行reduce端连接。具体如下：*\n* 01，首先创建一个模型，这里因为有订单和用户，所以这个模型要稍微复杂一些。一个模型中需要能判别类型，然后重点重写比较方法，分几种情况：不同的用户cid是升序还是降序。相同用户的不同订单，订单号是升序还是降序。最重要的是两个模型一个是用户，一个是订单的时候，一定要把用户放在最前面，方便后续reduce中取出第一个就是用户。\n\n* 02，然后是分组，相同的cid分到一组即可。然后在每组数据中取出第一个就是用户模型，剩下的就是这个用户下面的订单模型。明白了吧，具体看代码\n\n### 01，IMJoinKey最重要，先看模型\n\n```java\npackage im.ivanl001.bigData.Hadoop.A18_ReducerJoin;\n\nimport org.apache.hadoop.io.WritableComparable;\n\nimport java.io.DataInput;\nimport java.io.DataOutput;\nimport java.io.IOException;\n\n/**\n * #author      : ivanl001\n * #creator     : 2018-11-01 18:31\n * #description : 自定义key\n **/\npublic class IMJoinKey implements WritableComparable<IMJoinKey> {\n\n    //0 customer, 1 order\n    private int type;\n    private int cid ;\n    private int oid = -1;\n    //这里地方要有一个初始化的值，因为最后要写入写出，不能为null，不然会报错\n    private String userInfo = \"\";\n    private String orderInfo = \"\";\n\n\n    public int compareTo(IMJoinKey o) {\n\n        int otherType = o.getType();\n        int otherCid = o.getCid();\n        int otherOid = o.getOid();\n\n        String otherUserInfo = o.getUserInfo();\n        String otherOrderInfo = o.getOrderInfo();\n\n        if (this.cid == otherCid) {\n            //说明是同一个用户的\n            if (this.type == otherType) {\n                //说明是同一个用户的不同订单\n                return this.oid-otherOid;\n\n            } else {\n                //说明一个是用户，一个是订单，(因为类型不同)\n                if (type == 0) {\n                    //这里把用户放在最前面，方便后续直接取出组里面第一个进行操作。\n                    return -1;//把用户放在前面\n                }else {\n                    return 1;//还是把用户放在前面\n                }\n            }\n        }else{\n            //说明是不同的用户的信息\n            return this.cid - otherCid;\n        }\n    }\n\n    public void write(DataOutput out) throws IOException {\n        out.writeInt(type);\n        out.writeInt(cid);\n        out.writeInt(oid);\n        out.writeUTF(userInfo);\n        out.writeUTF(orderInfo);\n    }\n\n    public void readFields(DataInput in) throws IOException {\n        this.type = in.readInt();\n        this.cid = in.readInt();\n        this.oid = in.readInt();\n        this.userInfo = in.readUTF();\n        this.orderInfo = in.readUTF();\n    }\n\n\n    @Override\n    public String toString() {\n        return \"IMJoinKey{\" +\n                \"type=\" + type +\n                \", cid=\" + cid +\n                \", oid=\" + oid +\n                \", userInfo='\" + userInfo + '\\'' +\n                \", orderInfo='\" + orderInfo + '\\'' +\n                '}';\n    }\n\n    //get和set方法为了简洁，我这里直接删除了。\n}\n```\n\n### 02，IMJoinKeyComparator比较类\n\n```java\npackage im.ivanl001.bigData.Hadoop.A18_ReducerJoin;\n\nimport org.apache.hadoop.io.WritableComparable;\nimport org.apache.hadoop.io.WritableComparator;\n\n/**\n * #author      : ivanl001\n * #creator     : 2018-11-01 18:58\n * #description :\n **/\npublic class IMJoinKeyComparator extends WritableComparator {\n\n    protected IMJoinKeyComparator(){\n        super(IMJoinKey.class, true);\n    }\n\n    @Override\n    public int compare(WritableComparable a, WritableComparable b) {\n        IMJoinKey joinKey01 = (IMJoinKey) a;\n        IMJoinKey joinKey02 = (IMJoinKey) b;\n        return joinKey01.compareTo(joinKey02);\n    }\n}\n```\n\n### 03，IMCidPartitioner分区类\n```java\npackage im.ivanl001.bigData.Hadoop.A18_ReducerJoin;\n\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.mapreduce.Partitioner;\n\n/**\n * #author      : ivanl001\n * #creator     : 2018-11-01 18:47\n * #description : 根据用户id，cid进行分区，决定这个用户的相关信息在哪个r上执行\n *               这里的输入就是mapper的输出\n **/\npublic class IMCidPartitioner extends Partitioner<IMJoinKey, NullWritable> {\n\n    public int getPartition(IMJoinKey imJoinKey, NullWritable nullWritable, int numPartitions) {\n        int cid = imJoinKey.getCid();\n        System.out.println(\"=======================\"+numPartitions);\n        return cid%numPartitions;\n    }\n}\n```\n\n### 04，IMCidGroupSeparator分组类\n```java\npackage im.ivanl001.bigData.Hadoop.A18_ReducerJoin;\n\nimport org.apache.hadoop.io.WritableComparable;\nimport org.apache.hadoop.io.WritableComparator;\n\n/**\n * #author      : ivanl001\n * #creator     : 2018-11-01 18:50\n * #description : 根据cid进行分区，因为一个r上会有多个cid同时存在，那么这些cid如何进行进行就需要根据这个类\n **/\npublic class IMCidGroupSeparator extends WritableComparator {\n\n    protected IMCidGroupSeparator(){\n        super(IMJoinKey.class, true);\n    }\n\n    @Override\n    public int compare(WritableComparable a, WritableComparable b) {\n\n        IMJoinKey joinKey01 = (IMJoinKey) a;\n        IMJoinKey joinKey02 = (IMJoinKey) b;\n\n        return joinKey01.getCid()-joinKey02.getCid();\n    }\n}\n```\n\n### 05，IMJoinMapper\n```java\npackage im.ivanl001.bigData.Hadoop.A18_ReducerJoin;\n\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.lib.input.FileSplit;\n\nimport java.io.IOException;\n\n/**\n * #author      : ivanl001\n * #creator     : 2018-11-01 18:27\n * #description : 如果在reduce端join的话，也就相当于二次排序了\n **/\npublic class IMJoinMapper extends Mapper<LongWritable, Text, IMJoinKey, NullWritable> {\n\n    @Override\n    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n\n        //1, 首先需要根据输入判断是customer还是order\n        FileSplit fileSplit = (FileSplit) context.getInputSplit();\n        String path = fileSplit.getPath().toString();\n        IMJoinKey joinKey = new IMJoinKey();\n\n        String line = value.toString();\n\n        if (path.contains(\"customers\")) {\n\n            //是用户\n            joinKey.setType(0);\n            String cid = line.substring(0, line.indexOf(\",\"));\n            String userInfo = line.substring(line.indexOf(\",\") + 1);\n\n            joinKey.setCid(Integer.parseInt(cid));\n            joinKey.setUserInfo(userInfo);\n\n            System.out.println(joinKey);\n\n        }else {\n            //是订单\n            joinKey.setType(1);\n            String oid = line.substring(0, line.indexOf(\",\"));\n            String orderInfo = line.substring(0, line.lastIndexOf(\",\"));\n            String cid = line.substring(line.lastIndexOf(\",\") + 1);\n\n            joinKey.setOid(Integer.parseInt(oid));\n            joinKey.setCid(Integer.parseInt(cid));\n            joinKey.setOrderInfo(orderInfo);\n\n            System.out.println(joinKey);\n\n        }\n\n        //2, 直接把上面的信息写出即可\n        context.write(joinKey, NullWritable.get());\n    }\n}\n```\n\n### 06，IMJoinReducer\n```java\npackage im.ivanl001.bigData.Hadoop.A18_ReducerJoin;\n\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\nimport java.io.IOException;\nimport java.util.Iterator;\n\n/**\n * #author      : ivanl001\n * #creator     : 2018-11-01 19:16\n * #description :\n **/\npublic class IMJoinReducer extends Reducer<IMJoinKey, NullWritable, Text, NullWritable> {\n\n    @Override\n    protected void reduce(IMJoinKey key, Iterable<NullWritable> values, Context context) throws IOException, InterruptedException {\n\n        Iterator<NullWritable> iterator = values.iterator();\n        iterator.next();\n\n        //因为排序规则默认是把用户放在第一个的，所以直接把第一个取出来即可\n        int type_c = key.getType();\n        int cid_c = key.getCid();\n        String userInfo = key.getUserInfo();\n        System.out.println(\"====================================================================================\");\n        System.out.println(key);\n\n        int i = 0;\n        while (iterator.hasNext()) {\n            iterator.next();\n            i += 1;\n\n            int type_o = key.getType();\n            int oid = key.getOid();\n            int cid_o= key.getCid();\n            String orderInfo = key.getOrderInfo();\n\n            String outStr = cid_c + \"----\" + cid_o + \"----\" + userInfo + \"----\" + oid + \"----\" + orderInfo;\n            System.out.println(\"++++++++++++++++++++++++++++++++++----\"+i+\"----++++++++++++++++++++++++++++++++++\");\n            System.out.println(key);\n            context.write(new Text(outStr), NullWritable.get());\n        }\n    }\n}\n```\n\n### 7,IMReducerJoinApp最后的app\n```java\npackage im.ivanl001.bigData.Hadoop.A18_ReducerJoin;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\n/**\n * #author      : ivanl001\n * #creator     : 2018-11-01 19:24\n * #description :\n **/\npublic class IMReducerJoinApp {\n\n    public static void main(String[] args) throws Exception {\n\n        //1，先判断参数，如果通过则取出参数\n        if (args.length != 2) {\n            System.out.println(\"参数个数有误！\");\n            return;\n        }\n        String inputFileStr = args[0];\n        String outputFolderStr = args[1];\n\n\n        //2，获取文件系统，并删除输出文件夹以免出错\n        Configuration configuration = new Configuration();\n        FileSystem fileSystem = FileSystem.get(configuration);\n        fileSystem.delete(new Path(outputFolderStr));\n\n\n        //3,创建作业, 并设置基本的选项\n        Job maxTempJob = Job.getInstance(configuration);\n        maxTempJob.setJobName(\"reduceJoinApp\");\n        maxTempJob.setJarByClass(IMReducerJoinApp.class);\n        //因为采用的是序列文件，所以这里需要设置一下\n        maxTempJob.setInputFormatClass(TextInputFormat.class);\n\n\n        //4, 输入\n        FileInputFormat.addInputPath(maxTempJob, new Path(inputFileStr));\n\n\n        //5, 设置mapper\n        maxTempJob.setMapperClass(IMJoinMapper.class);\n        //这里其实是有点蠢的，其实在IMMaxTempMapper中已经指定了输出的key和value的类型了，但是这里还是需要指定一下，要不然就有可能不对\n        maxTempJob.setMapOutputKeyClass(IMJoinKey.class);\n        maxTempJob.setMapOutputValueClass(NullWritable.class);\n\n\n        //6, 设置reducer\n        //注意：如果有多个r的话(一般都会有多个r)，那么这样的就不属于全排序了，因为不同的年份会分布在不同的r中处理，也就会有多个文件输出\n        maxTempJob.setReducerClass(IMJoinReducer.class);\n        maxTempJob.setNumReduceTasks(2);\n\n\n\n        //设置分区函数，自定义分区是ok的，那我们再试试用全排序加采样进行试试\n        maxTempJob.setPartitionerClass(IMCidPartitioner.class);\n\n\n        //排序和分组都是在mapper之后进行的，先进行排序，排序之后在进行分组\n\n\n\n        //设置排序\n        maxTempJob.setSortComparatorClass(IMJoinKeyComparator.class);\n\n        //设置分组\n        maxTempJob.setGroupingComparatorClass(IMCidGroupSeparator.class);\n\n\n\n\n        //------------------------注意：采样要放在分区之后，否则会报错什么找不到分区文件，这种问题其实感觉应该是bug，不然为啥要放在后面，本来采样就是在r之前的啊------------------------------------\n        /*\n\n        //7.1, 设置分区函数,这里还是不能用自己定义的分区函数，因为你用自己定义的分区函数，采样的意义也就没有了\n        //maxTempJob.setPartitionerClass(IMYearPatitioner.class);\n        maxTempJob.setPartitionerClass(TotalOrderPartitioner.class);\n        //如果需要指定位置的话，需要注意一下：设置的时候使用configuration是不能直接使用的，而是需要从maxTempJob.getConfiguration()取出来使用，因为job里面是重新拷贝了一份\n        //TotalOrderPartitioner.setPartitionFile(maxTempJob.getConfiguration(), new Path(\"/Users/ivanl001/Desktop/bigData/output005/partitioner.lst\"));\n\n        //7.2, 设置采样\n        //说明一下，因为采样是对key进行采样的，所以只好用seqfile，要不然比如说用的是文本文件格式， key是偏移量，采样就没有什么意义了\n        //这句话暂时不知道为什么要设置，先注释\n        //maxTempJob.setPartitionerClass(TotalOrderPartitioner.class);\n        InputSampler.Sampler<IntWritable, IntWritable> sampler = new InputSampler.RandomSampler<IntWritable, IntWritable>(0.1, 10000, 10);\n        InputSampler.writePartitionFile(maxTempJob, sampler);\n\n        //----------呀呀呀呀，采样真的很好用哈，哈哈哈哈---------------\n\n        *//*Configuration conf = maxTempJob.getConfiguration();\n        String partitionFile = TotalOrderPartitioner.getPartitionFile(conf);\n        URI partitionUri = new URI(partitionFile);\n        maxTempJob.addCacheFile(partitionUri);*//*\n         */\n\n        //7，输出\n        FileOutputFormat.setOutputPath(maxTempJob, new Path(outputFolderStr));\n\n        //8, 提交，开始处理\n        maxTempJob.waitForCompletion(false);\n    }\n}\n```\n\n!!! 完结撒花  !!!\n"
    }
  ]
}