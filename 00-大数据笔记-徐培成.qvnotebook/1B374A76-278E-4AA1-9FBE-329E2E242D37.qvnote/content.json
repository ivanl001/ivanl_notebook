{
  "title": "10-Storm-0203-Storm数据倾斜解决",
  "cells": [
    {
      "type": "markdown",
      "data": "*其实数据倾斜的问题很好解决，还是拿wordcount来举例，第一次算和的时候采用shuffle随机分组，然后第二次再采用一次field分组方式，最后一个bolt来收集计算的数据即可，具体代码参考如下：*\n```\n一共有六个类：\nWordCountApp类：    主app入口\nWordCountSpout类：  输入\nWordCountSplitBolt：语句切割bolt节点，\nWordCountSumBolt：  这个计算总和的类中，需要注意的是解决数据倾斜需要用到两次这个类，一次用shuffle随机分组，一次用field分组进行单个单词的汇总\nWordCountResultBolt: 在这里进行最后数据的搜集\n```"
    },
    {
      "type": "markdown",
      "data": "####  WordCountApp类\n```java\npackage im.ivanl001.bigData.Storm.A03_WordCount_02_secondary;\n\nimport org.apache.storm.Config;\nimport org.apache.storm.LocalCluster;\nimport org.apache.storm.StormSubmitter;\nimport org.apache.storm.generated.AlreadyAliveException;\nimport org.apache.storm.generated.AuthorizationException;\nimport org.apache.storm.generated.InvalidTopologyException;\nimport org.apache.storm.topology.TopologyBuilder;\nimport org.apache.storm.tuple.Fields;\n\n/**\n * #author      : ivanl001\n * #creator     : 2018-11-17 09:09\n * #description : 单词统计app， 解决数据倾斜问题:这里使用的是二次聚合的方法，比较不同的是：WordCountSumBolt中的值需要传递出去，给第二次的聚合使用\n *\n * ??但是应该好是有一个问题的：在sumBolt中把结果发送出去后就把map清空了， 那结果怎么搜集？(意思是WordCountSumBolt要使用两次，第一次随机分组，第二次才按字段分组，这样可以规避数据倾斜，但是WordCountSumBolt既然要用两次，那肯定是要把数据发送出去，第二次再使用的时候才能接受对吧，但是第二次使用的时候代码中也有发送出去的代码，在发送出去的时候刚好也把map清空了， 那结果map的数据怎么能搜集到呢？)\n *\n * 后面想到一个办法，就是重新再写一个bolt最后接受数据就好了，笨\n *\n **/\npublic class WordCountApp {\n\n    public static void main(String[] args) throws InterruptedException, InvalidTopologyException, AuthorizationException, AlreadyAliveException {\n\n        Config config = new Config();\n        config.setDebug(false);\n        //这个是设置开启几个worker\n        config.setNumWorkers(2);\n\n        TopologyBuilder builder = new TopologyBuilder();\n\n        //parallelism_hint是设置并发的数量，比如下面spout设置成2,那么会开两个线程，跑3个任务，这个时候一个线程就必须同时跑两个任务，这其实不太好，所以并发数需要尽量大于任务数\n        builder.setSpout(\"wordCountSpout\", new WordCountSpout(), 2).setNumTasks(2);\n\n        builder.setBolt(\"wordCountSplitBolt\", new WordCountSplitBolt(), 3).shuffleGrouping(\"wordCountSpout\").setNumTasks(3);\n        //计算总数的时候为了解决数据倾斜的问题，这里需要进行两次计算总数，第一次混洗分组，第二次根据字段分组\n        builder.setBolt(\"wordCountSumBolt_shuffle\", new WordCountSumBolt(), 4).shuffleGrouping(\"wordCountSplitBolt\").setNumTasks(4);\n        builder.setBolt(\"wordCountSumBolt_field\", new WordCountSumBolt(), 1).fieldsGrouping(\"wordCountSumBolt_shuffle\", new Fields(\"word\")).setNumTasks(1);\n        builder.setBolt(\"wordCountResultBolt\", new WordCountResultBolt(), 1).fieldsGrouping(\"wordCountSumBolt_field\", new Fields(\"word\")).setNumTasks(1);//最后搜集\n\n        //这里用本地模式来测试分区模式，消息还是发送到master上的nc好了，懒得改了，一样看\n        /*LocalCluster localCluster = new LocalCluster();\n        localCluster.submitTopology(\"localWordCountCluster\", config, builder.createTopology());\n        Thread.sleep(10000);\n        localCluster.shutdown();*/\n\n        //集群提交\n        StormSubmitter.submitTopology(\"wordCountCluster\", config, builder.createTopology());\n    }\n}\n```"
    },
    {
      "type": "markdown",
      "data": "####  WordCountSpout类\n```java\npackage im.ivanl001.bigData.Storm.A03_WordCount_02_secondary;\n\nimport im.ivanl001.bigData.IMUtils.IMTrackerUtils;\nimport org.apache.storm.spout.SpoutOutputCollector;\nimport org.apache.storm.task.TopologyContext;\nimport org.apache.storm.topology.IRichSpout;\nimport org.apache.storm.topology.OutputFieldsDeclarer;\nimport org.apache.storm.tuple.Fields;\nimport org.apache.storm.tuple.Values;\n\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Random;\n\n/**\n * #author      : ivanl001\n * #creator     : 2018-11-17 09:09\n * #description : 单词统计的源\n **/\npublic class WordCountSpout implements IRichSpout {\n\n    //在这里定义一些输入，方便后面的Bolt进行计算\n    //private TopologyContext context;\n    private SpoutOutputCollector collector;\n    private List<String> words;\n    private Random random;\n\n    public void open(Map map, TopologyContext topologyContext, SpoutOutputCollector spoutOutputCollector) {\n\n        IMTrackerUtils.writeTo(this, \"WordCountSpout----open\", \"master\", 8880);\n\n        //this.context = topologyContext;\n        this.collector = spoutOutputCollector;\n        words = new ArrayList<String>();\n\n        words.add(\"ivanl001 is the king of world!\");\n        words.add(\"ivanl002 is the king of world!\");\n        words.add(\"ivanl003 is the king of world!\");\n        words.add(\"ivanl004 is the king of world!\");\n        words.add(\"ivanl005 is the king of world!\");\n        words.add(\"ivanl006 is the king of world!\");\n\n        random = new Random();\n    }\n\n    public void close() {\n\n    }\n\n    public void activate() {\n\n    }\n\n    public void deactivate() {\n\n    }\n\n    public void nextTuple() {\n\n        IMTrackerUtils.writeTo(this, \"WordCountSpout----nextTuple\", \"master\", 8880);\n\n        System.out.println(\"---------------------------nextTuple----------------------------\");\n\n        String line = words.get(random.nextInt(words.size()));\n        collector.emit(new Values(line));\n\n        //这里为了防止发送太快，延迟一下, 已经限制数量了，这里无所谓了\n        /*try {\n            Thread.sleep(1000);\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        }*/\n    }\n\n    public void ack(Object o) {\n\n    }\n\n    public void fail(Object o) {\n\n    }\n\n    public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) {\n        outputFieldsDeclarer.declare(new Fields(\"line\"));\n    }\n\n    public Map<String, Object> getComponentConfiguration() {\n        return null;\n    }\n}\n```"
    },
    {
      "type": "markdown",
      "data": "#### WordCountSplitBolt类\n```java\npackage im.ivanl001.bigData.Storm.A03_WordCount_02_secondary;\n\nimport im.ivanl001.bigData.IMUtils.IMTrackerUtils;\nimport org.apache.storm.task.OutputCollector;\nimport org.apache.storm.task.TopologyContext;\nimport org.apache.storm.topology.IRichBolt;\nimport org.apache.storm.topology.OutputFieldsDeclarer;\nimport org.apache.storm.tuple.Fields;\nimport org.apache.storm.tuple.Tuple;\nimport org.apache.storm.tuple.Values;\n\nimport java.util.Map;\n\n/**\n * #author      : ivanl001\n * #creator     : 2018-11-17 09:21\n * #description : 分裂语句节点\n **/\npublic class WordCountSplitBolt implements IRichBolt {\n\n    //private TopologyContext context;\n    private OutputCollector collector;\n    private int num = 0;\n\n    public void prepare(Map map, TopologyContext topologyContext, OutputCollector outputCollector) {\n\n        IMTrackerUtils.writeTo(this, \"WordCountSplitBolt----prepare\", \"master\", 8881);\n\n        //this.context = topologyContext;\n        this.collector = outputCollector;\n    }\n\n\n    //在这里执行计算操作\n    public void execute(Tuple tuple) {\n\n        //这里只让发送五行\n        if (num < 5) {\n\n            //这里可以通过字段名获取，也可以通过索引获取\n            String line = tuple.getString(0);\n            IMTrackerUtils.writeTo(this, \"WordCountSplitBolt----execute\" + \":::\" + line, \"master\", 8881);\n\n            String[] words = line.split(\" \");\n            for (String word : words) {\n                collector.emit(new Values(word, 1));\n            }\n\n            num += 1;\n        }\n    }\n\n    public void cleanup() {\n\n    }\n\n    public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) {\n        outputFieldsDeclarer.declare(new Fields(\"word\", \"count\"));\n    }\n\n    public Map<String, Object> getComponentConfiguration() {\n        return null;\n    }\n}\n```"
    },
    {
      "type": "markdown",
      "data": "#### WordCountSumBolt\n```java\npackage im.ivanl001.bigData.Storm.A03_WordCount_02_secondary;\n\nimport im.ivanl001.bigData.IMUtils.IMTrackerUtils;\nimport org.apache.storm.task.OutputCollector;\nimport org.apache.storm.task.TopologyContext;\nimport org.apache.storm.topology.IRichBolt;\nimport org.apache.storm.topology.OutputFieldsDeclarer;\nimport org.apache.storm.tuple.Fields;\nimport org.apache.storm.tuple.Tuple;\nimport org.apache.storm.tuple.Values;\n\nimport java.util.Collections;\nimport java.util.HashMap;\nimport java.util.Map;\n\n/**\n * #author      : ivanl001\n * #creator     : 2018-11-17 09:26\n * #description : 计算总数Bolt节点\n **/\npublic class WordCountSumBolt implements IRichBolt {\n\n    //private TopologyContext context;\n    private OutputCollector collector;\n\n    Map<String, Integer> result;\n\n    public void prepare(Map map, TopologyContext topologyContext, OutputCollector outputCollector) {\n\n        IMTrackerUtils.writeTo(this, \"WordCountSumBolt----prepare\", \"master\", 8882);\n\n        //this.context = topologyContext;\n        this.collector = outputCollector;\n        result = new HashMap<String, Integer>();\n        //转成线程安全的，方便后面另起线程清空操作等\n        result = Collections.synchronizedMap(result);\n\n        //在这里开启一个线程循环发送数据，因为如果只是在execute中发送的时候，有可能前面不发送数据，execute就不被触发，有一部分数据就发送不出去了\n        Thread thread = new Thread(){\n            @Override\n            public void run() {\n                //这里需要一直循环下去，一旦线程歇菜就完蛋了哈\n                while (true) {\n                    emitData();\n                }\n            }\n        };\n        //这个进行会循环，如果直接start只有一次，不是特别懂\n        thread.setDaemon(true);\n        thread.start();\n    }\n\n    //在这个线程中进行清理数据发送给下以阶段\n    public void emitData(){\n\n        //其实这里还是有一点不明白的，这里用result加锁了， 那么result是不是线程安全的是不是就无所谓了？？？\n        synchronized (result) {\n            //这里的result集合是HashMap， 是非线程安全的，所以为了线程问题，需要转换成线程安全的\n            for (Map.Entry<String, Integer> entry : result.entrySet()) {\n                collector.emit(new Values(entry.getKey(), entry.getValue()));\n                System.out.println(\"发送数据啦，数据个数是：\" + result.size());\n            }\n            result.clear();\n        }\n\n        try {\n            //这里一定要休眠一下，要不然会一直调用，很浪费\n            Thread.sleep(5000);\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        }\n    }\n\n    //在这里计算单词总数\n    public void execute(Tuple tuple) {\n\n        String word = tuple.getStringByField(\"word\");\n        Integer count = tuple.getIntegerByField(\"count\");\n\n        IMTrackerUtils.writeTo(this, \"WordCountSumBolt----execute\" + \":::\" + word, \"master\", 8882);\n\n        if (result.containsKey(word)) {\n            //已经包含了这个单词,那么取出原先的数量，加上本次数量\n            result.put(word, result.get(word) + count);\n        } else {\n            result.put(word, count);\n        }\n        //collector.ack(tuple);\n\n    }\n\n    public void cleanup() {\n\n        //这里再统计就没有意义了，在WordCountResultBolt中进行搜集统计数据\n        /*System.out.println(\"统计结束，开始打印结果-------------------\");\n        for (Map.Entry<String, Integer> entry:result.entrySet()){\n            System.out.println(entry.getKey() + \":\" + entry.getValue());\n        }*/\n    }\n\n    public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) {\n        outputFieldsDeclarer.declare(new Fields(\"word\", \"count\"));\n    }\n\n    public Map<String, Object> getComponentConfiguration() {\n        return null;\n    }\n}\n```"
    },
    {
      "type": "markdown",
      "data": "#### WordCountResultBolt类\n```java\npackage im.ivanl001.bigData.Storm.A03_WordCount_02_secondary;\n\nimport org.apache.storm.task.OutputCollector;\nimport org.apache.storm.task.TopologyContext;\nimport org.apache.storm.topology.IRichBolt;\nimport org.apache.storm.topology.OutputFieldsDeclarer;\nimport org.apache.storm.tuple.Tuple;\n\nimport java.util.HashMap;\nimport java.util.Map;\n\n/**\n * #author      : ivanl001\n * #creator     : 2018-11-20 10:21\n * #description : 这里是为了搜集到数据，因为之前的bolt都把数据发送出去了，所以必须要找个bolt接受一下然后搜集起来对吧\n **/\npublic class WordCountResultBolt implements IRichBolt {\n\n    Map<String, Integer> result;\n\n    public void prepare(Map map, TopologyContext topologyContext, OutputCollector outputCollector) {\n        result = new HashMap<String, Integer>();\n    }\n\n    public void execute(Tuple tuple) {\n        String word = tuple.getStringByField(\"word\");\n        Integer count = tuple.getIntegerByField(\"count\");\n        result.put(word, count);\n    }\n\n\n    public void cleanup() {\n        System.out.println(\"统计结束，开始打印结果-------------------\");\n        for (Map.Entry<String, Integer> entry:result.entrySet()){\n            System.out.println(entry.getKey() + \":\" + entry.getValue());\n        }\n    }\n\n    public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) {\n\n    }\n\n    public Map<String, Object> getComponentConfiguration() {\n        return null;\n    }\n}\n```"
    }
  ]
}